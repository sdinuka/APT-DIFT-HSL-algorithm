{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Sample code of value iteration algorithm (Algorithm 6.2 in [1]) and Hierarchical Supervised Learning (HSL) algorithm}$ $\\textbf{(Algorithm 6.3 in [1]) to find Nash Equilibrium in constant sum, stochastic, } \\textbf{Advanced Persistent Threats-Dynamic Information Flow}$ $\\textbf{Tracking (APT-DIFT) games}$\n",
    "\n",
    "$\\textbf{Code Description:}$\n",
    "\n",
    "(1) Python Version: 3.7\n",
    "\n",
    "(2) Keras Version: 2.3.1\n",
    "\n",
    "(3) This code takes as input Ransomeware_Data.mat file. Ransomeware_Data.mat is a data file containing state space of APT-DIFT game extracted from Ransomware [2] system logs.\n",
    "\n",
    "(4) For detailed explanation of the value iteration algorithm and APT-DIFT game please refer to [1].\n",
    "\n",
    "$\\textbf{Note:}$ You may freely redistribute and use this sample code, with or without modification, provided you include the original Copyright notice and use restrictions.\n",
    "\n",
    "$\\textbf{Disclaimer}:$ THE SAMPLE CODE IS PROVIDED \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL DINUKA SAHABANDU OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) SUSTAINED BY YOU OR A THIRD PARTY, HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT ARISING IN ANY WAY OUT OF THE USE OF THIS SAMPLE CODE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "    \n",
    "For additional information, contact:\n",
    "Dinuka Sahabandu, email: sdinuka@uw.edu\n",
    "\n",
    "Acknowledgement: This work was supported by ONR grant N00014-16-1-2710 P00002, DARPA TC grant DARPA FA8650-15-C-7556, and ARO grant W911NF-16-1-0485.\n",
    "[1] Shana Moothedath, Dinuka Sahabandu, Joey Allen, Linda Bushnell, Wenke Lee, and Radha Poovendran, â€œStochastic Dynamic Information Flow Tracking Game using Supervised Learning for Detecting Advanced Persistent Threats\".\n",
    "     - Arxiv link: https://arxiv.org/pdf/2007.12327.pdf\n",
    "     - Website: https://adapt.ece.uw.edu/\n",
    "     \n",
    "[2] Ji, Yang, Sangho Lee, Mattia Fazzini, Joey Allen, Evan Downing, Taesoo Kim, Alessandro Orso, and Wenke Lee. \"Enabling refinable cross-host attack investigation with efficient data flow tagging and tracking.\" In 27th USENIX Security Symposium (USENIX Security 18), pp. 1705-1722. 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from scipy.optimize import linprog\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from numpy.random import rand\n",
    "from scipy.io import loadmat\n",
    "from keras.models import load_model\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainingData:\n",
    "    entry_points = None\n",
    "    destination = None\n",
    "    FN = None\n",
    "    FP = None\n",
    "    beta = None\n",
    "    IFG = None\n",
    "    SS = None\n",
    "    Action_set_DIFT = None\n",
    "    Action_set_APT = None\n",
    "    intermediate_destinations = None\n",
    "\n",
    "\n",
    "    def __init__(self,entry_points,destination,intermediate_destinations,FN,FP,beta,IFG,SS,Action_set_DIFT,Action_set_APT):\n",
    "        self.entry_points = entry_points\n",
    "        self.destination = destination\n",
    "        self.FN = FN\n",
    "        self.FP = FP\n",
    "        self.beta = beta\n",
    "        self.IFG = IFG\n",
    "        self.SS = SS\n",
    "        self.Action_set_DIFT = Action_set_DIFT\n",
    "        self.Action_set_APT = Action_set_APT\n",
    "        self.intermediate_destinations = intermediate_destinations\n",
    "\n",
    "    def Generate_Training_Data(self,training_sets, pecent_stochastic_DIFT, pecent_stochastic_APT):\n",
    "        IFG_length = len(self.IFG)\n",
    "        SS_length = len(self.SS)\n",
    "\n",
    "        DIFT_Policies_Data = []\n",
    "        APT_Policies_Data = []\n",
    "        \n",
    "        V = np.zeros((training_sets, SS_length))\n",
    "        V[:, IFG_length + 1] = self.beta\n",
    "        V[:, IFG_length + 3] = self.beta\n",
    "\n",
    "        Input_vector_NN = [] #Input to the NN (i.e., DIFT policy + APT policy corresponding to each time ts)\n",
    "        Output_Data_NN = [] #Output of the NN (i.e., Value vector)\n",
    "             \n",
    "        \n",
    "        for ts in range(0,training_sets):\n",
    "            #seed(1)\n",
    "            \n",
    "            policy_type_DIFT = 0 #Type of DIFT policy :- Type 0: Deterministic, Type 1:Stochastic \n",
    "            uniform_var_DIFT = np.random.uniform(0,1,1)\n",
    "            if pecent_stochastic_DIFT >= uniform_var_DIFT*100:\n",
    "                policy_type_DIFT = 1\n",
    "                \n",
    "            #Generating random policies for DIFT\n",
    "            DIFT_Policy = []\n",
    "            \n",
    "            if policy_type_DIFT == 0:\n",
    "                for ii in range(0, IFG_length+1): #IFG states + Virtual state\n",
    "                    if len(self.Action_set_DIFT[ii]) != 1:\n",
    "                        random_vec_DIFT = np.zeros(len(self.Action_set_DIFT[ii]))\n",
    "                        DIFT_action = np.random.randint(len(self.Action_set_DIFT[ii]), size=1)\n",
    "                        random_vec_DIFT[DIFT_action] = 1\n",
    "                        DIFT_Policy.append((random_vec_DIFT).tolist()) \n",
    "                    else:\n",
    "                        DIFT_Policy.append([1])\n",
    "            else:\n",
    "                for ii in range(0, IFG_length+1): #IFG states + Virtual state\n",
    "                    if len(self.Action_set_DIFT[ii]) != 1:\n",
    "                        random_vec_DIFT = abs(rand(len(self.Action_set_DIFT[ii])))\n",
    "                        DIFT_Policy.append((random_vec_DIFT / sum(random_vec_DIFT)).tolist())\n",
    "                    else:\n",
    "                        DIFT_Policy.append([1])\n",
    "                        \n",
    "            \n",
    "            policy_type_APT = 0 #Type of APT policy :- Type 0: Deterministic, Type 1:Stochastic \n",
    "            uniform_var_APT = np.random.uniform(0,1,1)\n",
    "            if pecent_stochastic_APT >= uniform_var_APT*100:\n",
    "                policy_type_APT = 1\n",
    "            \n",
    "            \n",
    "            # Generating random policies for APT\n",
    "            APT_Policy = []\n",
    "            \n",
    "            if policy_type_APT == 0:\n",
    "                for ii in range(0, IFG_length+1): #IFG states + Virtual state\n",
    "                    if len(self.Action_set_APT[ii]) != 1:\n",
    "                        random_vec_APT = np.zeros(len(self.Action_set_APT[ii]))\n",
    "                        APT_action = np.random.randint(len(self.Action_set_APT[ii]), size=1)\n",
    "                        random_vec_APT[APT_action] = 1\n",
    "                        APT_Policy.append((random_vec_APT).tolist()) \n",
    "                    else:\n",
    "                        APT_Policy.append([1])\n",
    "            else:\n",
    "                for ii in range(0, IFG_length+1): #IFG states + Virtual state\n",
    "                    if len(self.Action_set_APT[ii]) != 1:\n",
    "                        random_vec_APT = abs(rand(len(self.Action_set_APT[ii])))\n",
    "                        APT_Policy.append((random_vec_APT / sum(random_vec_APT)).tolist())\n",
    "                    else:\n",
    "                        APT_Policy.append([1])\n",
    "                    \n",
    "\n",
    "            #Collect policy data into list\n",
    "            DIFT_Policies_Data.append(DIFT_Policy)\n",
    "            APT_Policies_Data.append(APT_Policy)\n",
    "\n",
    "            #Evaluating the valuye functions (Dynamic programing:Back propagation approach to evaluate value function)\n",
    "\n",
    "            #Convert IFG transition matrix to nx.DiGraph format\n",
    "            IFG_graph = nx.DiGraph()\n",
    "            for ii in range(IFG_length):\n",
    "                for jj in range(IFG_length):\n",
    "                    if self.IFG[ii, jj] == 1:\n",
    "                        IFG_graph.add_edge(ii, jj)\n",
    "            #Topological order for dynamic program (Only consider IFG node related states)\n",
    "            TP_Order = list(reversed(list(nx.topological_sort(IFG_graph))))\n",
    "            #Excluding destination node related states from state space\n",
    "            SS_To_Visit = TP_Order.copy()\n",
    "            for ii in range(0,len(TP_Order)):\n",
    "                dest_flag = TP_Order[ii] in self.destination.tolist()\n",
    "                inter_dest_flag = TP_Order[ii] in self.intermediate_destinations.tolist()\n",
    "                if int(dest_flag) == 1 or int(inter_dest_flag) == 1:\n",
    "                    pop_index = SS_To_Visit.index(TP_Order[ii])\n",
    "                    SS_To_Visit.pop(pop_index)\n",
    "\n",
    "            #Calculating the values at all non-absorbing states except at the virtual state\n",
    "            #V[ts, SS_To_Visit[ii]] = 0\n",
    "            for ii in range(0,len(SS_To_Visit)):\n",
    "                for dd in range(0,len(self.Action_set_DIFT[SS_To_Visit[ii]])):\n",
    "                    for aa in range(0,len(self.Action_set_APT[SS_To_Visit[ii]])):\n",
    "                        if aa == (len(self.Action_set_APT[SS_To_Visit[ii]]) - 1):  # APT dropout\n",
    "                            V[ts, SS_To_Visit[ii]] = V[ts, SS_To_Visit[ii]] + V[ts, IFG_length + 3]*DIFT_Policy[SS_To_Visit[ii]][dd]*APT_Policy[SS_To_Visit[ii]][aa]  # Next state is \\phi\n",
    "                            # print(t,ii,kk,Q)\n",
    "                        else:  # APT not dropped out\n",
    "                            if dd < (len(self.Action_set_DIFT[SS_To_Visit[ii]]) - 1):  # DIFT trap\n",
    "                                if dd == aa:  # Trap performed at the APT's state\n",
    "                                    # Two cases due to False negatives\n",
    "                                    V[ts, SS_To_Visit[ii]] = V[ts, SS_To_Visit[ii]] + ((1 - self.FN[self.Action_set_DIFT[SS_To_Visit[ii]][dd]])*V[ts, IFG_length + 1] + self.FN[self.Action_set_DIFT[SS_To_Visit[ii]][dd]]*V[ts, self.Action_set_APT[SS_To_Visit[ii]][aa]])*DIFT_Policy[SS_To_Visit[ii]][dd]*APT_Policy[SS_To_Visit[ii]][aa]\n",
    "                                    # Next state is APT's transition action +   \\tau_A\n",
    "                                else:  # unsuccessful trap due to different place of trapping\n",
    "                                    # Two cases due to False Positives\n",
    "                                    V[ts, SS_To_Visit[ii]] = V[ts, SS_To_Visit[ii]] + ((1 - self.FP[self.Action_set_DIFT[SS_To_Visit[ii]][dd]])*V[ts, self.Action_set_APT[SS_To_Visit[ii]][aa]] + self.FP[self.Action_set_DIFT[SS_To_Visit[ii]][dd]]*V[ts, IFG_length + 2])*DIFT_Policy[SS_To_Visit[ii]][dd]*APT_Policy[SS_To_Visit[ii]][aa]\n",
    "                                    # Next state is APT's transition action +   \\tau_B\n",
    "                            else:  # DIFT does not trap\n",
    "                                V[ts, SS_To_Visit[ii]] = V[ts, SS_To_Visit[ii]] + V[ts, self.Action_set_APT[SS_To_Visit[ii]][aa]]*DIFT_Policy[SS_To_Visit[ii]][dd]*APT_Policy[SS_To_Visit[ii]][aa]  # Next state is APT's transition action\n",
    "\n",
    "\n",
    "            #Calculating value at the virtual state\n",
    "            for aa in range(0, len(self.Action_set_APT[IFG_length])):\n",
    "                V[ts, IFG_length] = V[ts, IFG_length] + V[ts, self.Action_set_APT[IFG_length][aa]]*APT_Policy[IFG_length][aa] #Next state depends on the APT's transition\n",
    "            \n",
    "            #Setting values at the intermediate states \n",
    "            for ii in self.intermediate_destinations.tolist(): \n",
    "                neighbor = np.nonzero(self.IFG[ii, :])\n",
    "                V[ts, ii] = V[ts, neighbor[0]]\n",
    "\n",
    "        #Reshaping the Input vectors (DIFT + APT policy)\n",
    "\n",
    "            tmp_input_vec = []\n",
    "            for ii in range(0, len(DIFT_Policy) + len(APT_Policy)):\n",
    "                if ii < len(DIFT_Policy):\n",
    "                    num_actions_DIFT = len(DIFT_Policy[ii])\n",
    "                    if num_actions_DIFT == 0:\n",
    "                        tmp_input_vec = tmp_input_vec + [1]\n",
    "                    else:\n",
    "                        for kk in range(0,num_actions_DIFT):\n",
    "                            tmp_input_vec = tmp_input_vec + [DIFT_Policy[ii][kk]]\n",
    "                else:\n",
    "                    #Shift ii variable\n",
    "                    ii = ii - len(DIFT_Policy)\n",
    "                    num_actions_APT = len(APT_Policy[ii])\n",
    "                    if num_actions_APT == 0:\n",
    "                        tmp_input_vec = tmp_input_vec + [1]\n",
    "                    else:\n",
    "                        for kk in range(0,num_actions_APT):\n",
    "                            tmp_input_vec = tmp_input_vec + [APT_Policy[ii][kk]]\n",
    "\n",
    "            Input_vector_NN.append(tmp_input_vec)\n",
    "            \n",
    "        V = V.tolist()\n",
    "        Input_Output_Data = []\n",
    "        Input_Output_Data.append(Input_vector_NN)\n",
    "        Input_Output_Data.append(V)\n",
    "\n",
    "        return Input_Output_Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_NN_model():\n",
    "    \n",
    "    save_flag = 0\n",
    "    #Divide dataset into train, test and validation\n",
    "    num_vali = int(TD_len*0.1)\n",
    "    Policy_vali = Policy[0:num_vali]\n",
    "    Policy_test = Policy[num_vali:2*num_vali]\n",
    "    Policy_train = Policy[2*num_vali:]\n",
    "\n",
    "    Value_vali = -(Value[0:num_vali])\n",
    "    Value_test = -(Value[num_vali:2*num_vali])\n",
    "    Value_train = -(Value[2*num_vali:])\n",
    "    \n",
    "    #Define Neural Network \n",
    "    model = Sequential()\n",
    "    model.add(Dense(1000, input_dim=len(Policy_train[0]), activation='relu'))\n",
    "    model.add(Dense(1000, activation='relu'))\n",
    "    model.add(Dense(len(Value_train[0])))\n",
    "    \n",
    "    model_name = 'NN_trained.h5'\n",
    "\n",
    "    #Define parameters\n",
    "    batch_size = 128\n",
    "    num_training_epoch = 100\n",
    "    learning_rate = 2e-1\n",
    "    learning_rate_decay = learning_rate/(num_training_epoch)#1e-6\n",
    "    target_mae = 500 #precision in mean absolute error\n",
    "    epoch = 0\n",
    "\n",
    "    #Define Stochastic Gradient Descent class\n",
    "    sgd = SGD(lr=learning_rate, decay=learning_rate_decay, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='mean_absolute_error', optimizer=sgd, metrics=['mae'])\n",
    "\n",
    "    #Compute MSE (mean square error)  for trainig, validation and test data\n",
    "    _, train_mae = model.evaluate(Policy_train, Value_train, verbose=0)\n",
    "    _, vali_mae = model.evaluate(Policy_vali, Value_vali, verbose=0)\n",
    "    _, test_mae = model.evaluate(Policy_test, Value_test, verbose=0)\n",
    "    \n",
    "    #print(\"epoch: \" + str(epoch) + \", train mae: \" + str(np.round(1000*train_mae)) + \", vali mae: \" + str(np.round(1000*vali_mae)) + \", test mae: \" + str(np.round(1000*test_mae)))\n",
    "    \n",
    "    for epoch in range(num_training_epoch):\n",
    "    \n",
    "        #t1 = time.time()\n",
    "    \n",
    "        ##########################################################\n",
    "    \n",
    "        if epoch % 20 == 0:\n",
    "            learning_rate /= 2\n",
    "            sgd = SGD(lr=learning_rate, decay=learning_rate_decay, momentum=0.9, nesterov=True)\n",
    "            model.compile(loss='mean_absolute_error', optimizer=sgd, metrics=['mae'])\n",
    "    \n",
    "        ##########################################################\n",
    "    \n",
    "        #Fit model\n",
    "        model.fit(Policy_train, Value_train, batch_size=batch_size, epochs=1, verbose=0)\n",
    "    \n",
    "        ##########################################################\n",
    "    \n",
    "        _, train_mae = model.evaluate(Policy_train, Value_train, verbose=0)\n",
    "        _, vali_mae = model.evaluate(Policy_vali, Value_vali, verbose=0)\n",
    "        _, test_mae = model.evaluate(Policy_test, Value_test, verbose=0)\n",
    "       \n",
    "        ##########################################################\n",
    "        print(\"epoch: \" + str(epoch) + \", train mae: \" + str(np.round(1000*train_mae)) + \", vali mae: \" + str(np.round(1000*vali_mae)) + \", test mae: \" + str(np.round(1000*test_mae)))\n",
    "\n",
    "        if np.round(1000*vali_mae) <= target_mae:\n",
    "            model.save(model_name)\n",
    "            target_mae = np.round(1000*vali_mae)\n",
    "            print('Saved!')\n",
    "            save_flag = 1\n",
    "            #print(\"epoch: \" + str(epoch) + \", train mae: \" + str(np.round(1000*train_mae)) + \", vali mae: \" + str(np.round(1000*vali_mae)) + \", test mae: \" + str(np.round(1000*test_mae)))\n",
    "    if save_flag == 0:\n",
    "        model.save(model_name)\n",
    "        print('Saved-Not a good approximation!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stage_Game_solver(current_state,Q_est,V,estimate = False): \n",
    "    # Policy for DIFT (optimization variable in LP)\n",
    "    DIFT_Policy_current_state = np.zeros((len(Action_set_DIFT[current_state]))).tolist()\n",
    "    \n",
    "    # Define Linear Program\n",
    "    # Variable structure [p(s,d1), ..., p(s,dN), V(s)]\n",
    "\n",
    "    # Objective function\n",
    "    obj = np.zeros(len(DIFT_Policy_current_state)).tolist()\n",
    "    obj.extend([-1])  # last entry corresponding to value function at state current_state\n",
    "\n",
    "    # Linear equality constraints (Total one)\n",
    "    # Sum of DIFT's defense policy at each state current_state is one\n",
    "    lhs_eq = np.ones((1, len(DIFT_Policy_current_state) + 1)).tolist()  # +1 to represent V(s)\n",
    "    lhs_eq[0][-1] = 0  # last entry is \"0\" (no V(s) involved)\n",
    "    rhs_eq = [1]\n",
    "\n",
    "    # Linear inequality constraints (Total = # actions of DIFT at state current_state + # of actions of APT at state current_state\n",
    "    # Each entry is poliy of DIFT must be > 0 + For each APT's action V(s) <= \\sum_{d} Q(s,d,a)pD(s,d)\n",
    "    #print(len(Action_set_DIFT[current_state]),len(Action_set_APT[current_state]))\n",
    "    len_lhs_ineq = len(Action_set_DIFT[current_state]) + len(Action_set_APT[current_state])\n",
    "    lhs_ineq = np.zeros((len_lhs_ineq, len(DIFT_Policy_current_state) + 1)).tolist()\n",
    "    # print(current_state, lhs_ineq)\n",
    "    #Initializing exact Q values (in a matrix where rows: DIFT's actions, columns: APT's actions)\n",
    "    Q_values_exact = np.zeros((len(Action_set_DIFT[current_state]), len(Action_set_APT[current_state])))\n",
    "    for jj in range(0, len_lhs_ineq):\n",
    "        if jj < len(Action_set_DIFT[current_state]):  # constraint on the probability\n",
    "            # lhs_ineq[jj] = np.zeros(len(DIFT_Policy_current_state)+1).tolist()\n",
    "            lhs_ineq[jj][jj] = -1\n",
    "\n",
    "        else:  # constraints on Q and DIFT's policy\n",
    "            lhs_ineq[jj][-1] = 1  # V(s) position\n",
    "            kk = jj - len(Action_set_DIFT[current_state])  # APT action\n",
    "            for ll in range(0, len(Action_set_DIFT[current_state])):\n",
    "                if int(estimate) == 0: #Calculating exact Q values using the knowledge on FPs and FNs\n",
    "                    Q = 0\n",
    "                    if kk == (len(Action_set_APT[current_state]) - 1):  # APT dropout\n",
    "                        Q = V[IFG_length + 3]  # Next state is \\phi\n",
    "                        # print(t,ii,kk,Q)\n",
    "                    else:  # APT not dropped out\n",
    "                        if ll < (len(Action_set_DIFT[current_state]) - 1):  # DIFT trap\n",
    "                            if kk == ll:  # Trap performed at the APT's state\n",
    "                                # Two cases due to False negatives\n",
    "                                Q = (1 - FN[Action_set_DIFT[current_state][ll]]) * V[IFG_length + 1] + FN[\n",
    "                                    Action_set_DIFT[current_state][ll]] * V[Action_set_APT[current_state][kk]]\n",
    "                                # Next state is APT's transition action +   \\tau_A\n",
    "                            else:  # unsuccessful trap due to different place of trapping\n",
    "                                # Two cases due to False Positives\n",
    "                                Q = (1 - FP[Action_set_DIFT[current_state][ll]]) * V[Action_set_APT[current_state][kk]] + FP[\n",
    "                                    Action_set_DIFT[current_state][ll]] * V[IFG_length + 2]\n",
    "                                # Next state is APT's transition action +   \\tau_B\n",
    "                        else:  # DIFT does not trap\n",
    "                            Q = V[Action_set_APT[current_state][kk]]  # Next state is APT's transition action\n",
    "                            \n",
    "                    lhs_ineq[jj][ll] = -Q\n",
    "                    Q_values_exact[ll][kk] = Q\n",
    "                    \n",
    "                else:#Using Q values estimated using NN model\n",
    "                    Q = 0\n",
    "                    if kk == (len(Action_set_APT[current_state]) - 1):  # APT dropout\n",
    "                        Q = V[IFG_length + 3]  # Next state is \\phi\n",
    "                        Q_est[ll][kk] = -Q\n",
    "                    else:  # APT not dropped out\n",
    "                        if ll < (len(Action_set_DIFT[current_state]) - 1):  # DIFT trap\n",
    "                            if kk == ll:  # Trap performed at the APT's state\n",
    "                                # use estimated Q values since FN unknown\n",
    "                                Q = -Q_est[ll][kk]\n",
    "                                # Next state is APT's transition action +   \\tau_A\n",
    "                            else:  # unsuccessful trap due to different place of trapping\n",
    "                                # use estimated Q values since FP unknown\n",
    "                                Q = -Q_est[ll][kk]\n",
    "                                # Next state is APT's transition action +   \\tau_B\n",
    "                        else:  # DIFT does not trap\n",
    "                            Q = V[Action_set_APT[current_state][kk]]  # Next state is APT's transition action\n",
    "                            Q_est[ll][kk] = -Q            \n",
    "                    lhs_ineq[jj][ll] = -Q\n",
    "                    \n",
    "\n",
    "    rhs_ineq = np.zeros(len_lhs_ineq).tolist()\n",
    "    \n",
    "    # Solve LP\n",
    "    opt = linprog(c=obj, A_ub=lhs_ineq, b_ub=rhs_ineq, A_eq=lhs_eq, b_eq=rhs_eq,\n",
    "                  method=\"revised simplex\")\n",
    "    DIFT_Policy_tmp = opt.x[0:-1]\n",
    "    # Update DIFT's Policy\n",
    "    DIFT_Policy_current_state = DIFT_Policy_tmp.tolist()\n",
    "    # Update the value function\n",
    "    V_current_state = opt.x[-1]\n",
    "    \n",
    "    #Finding APT's NE policy\n",
    "    if int(estimate) == 0: #Based on exact Q values\n",
    "        payoff_vector_APT = (Q_values_exact.T).dot(DIFT_Policy_tmp.T)\n",
    "    else: #Based on estimated Q values\n",
    "        payoff_vector_APT = -(Q_est.T).dot(DIFT_Policy_tmp.T)\n",
    "        \n",
    "    APT_action_ID = (payoff_vector_APT.tolist()).index(min(payoff_vector_APT))\n",
    "    APT_Policy_tmp = np.zeros(len(Action_set_APT[current_state]))\n",
    "    APT_Policy_tmp[APT_action_ID] = 1\n",
    "    APT_Policy_current_state = APT_Policy_tmp.tolist()                         \n",
    "                              \n",
    "    \n",
    "    #Save output data (NE policies and NE value corresponding to the current state)\n",
    "    Policy_Values = []\n",
    "    Policy_Values.append(DIFT_Policy_current_state)\n",
    "    Policy_Values.append(APT_Policy_current_state)\n",
    "    Policy_Values.append(V_current_state)\n",
    "    \n",
    "    return Policy_Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration_main(exact_apprx_flag):\n",
    "\n",
    "    #Initializing the value vector of the game \n",
    "    V = np.zeros(SS_length)\n",
    "    V[IFG_length + 1] = beta\n",
    "    V[IFG_length + 3] = beta\n",
    "\n",
    "    #Random stochastic policies for DIFT and APT  \n",
    "    TrainingDataObj = trainingData(entry_points, destination, intermediate_destinations, FN, FP, beta, IFG, SS, Action_set_DIFT, Action_set_APT)\n",
    "    pecent_stochastic_DIFT = 40 #Specify percentage of stochastic policies generated for DIFT\n",
    "    pecent_stochastic_APT = 0 #Specify percentage of stochastic policies generated for APT\n",
    "    Training_Data_sample = TrainingDataObj.Generate_Training_Data(1,pecent_stochastic_DIFT,pecent_stochastic_APT)\n",
    "    Input_vector = np.copy(Training_Data_sample[0])\n",
    "\n",
    "    #Solving stage games using NN predicttions of \"Q\" values  \n",
    "    for ii in range(0,len(SS_To_Visit)):\n",
    "    \n",
    "        current_state = SS_To_Visit[ii]\n",
    "        \n",
    "        #Solving the stage game using LP with exact Q values if estimate = False\n",
    "        if exact_apprx_flag == 1:\n",
    "            Policy_Values = Stage_Game_solver(current_state,[],V,estimate = False)\n",
    "        elif exact_apprx_flag == 0:\n",
    "            num_DIFT_actions = len(Action_set_DIFT[current_state])\n",
    "            num_APT_actions = len(Action_set_APT[current_state])\n",
    "            Q_est = np.zeros((num_DIFT_actions, num_APT_actions))#Q table for each state, rows: DIFT's actions, Columns:APT's actions     \n",
    "    \n",
    "            #Estimating Q values using trained NN model \n",
    "            for dd in range(0,num_DIFT_actions):\n",
    "                for aa in range(0,num_APT_actions):\n",
    "                    #Constructing input vector for NN  to predict corresponding Q values at the \"current state\"\n",
    "                    #Deterministic policies at the current state\n",
    "                    PdPa_current_state = np.zeros((num_DIFT_actions+num_APT_actions))\n",
    "                    PdPa_current_state[dd] = 1\n",
    "                    PdPa_current_state[aa+num_DIFT_actions] = 1\n",
    "                    PdPa_current_state.tolist()\n",
    "            \n",
    "                    #Identifying the locations in Input_vector where PdPa_current_state needs to ve substituted \n",
    "                    start_ptr_DIFT_policy = 0 \n",
    "                    end_ptr_DIFT_policy = 0\n",
    "                    start_ptr_APT_policy = 0 \n",
    "                    end_ptr_APT_policy = 0\n",
    "                    Total_Actions_DIFT = 0\n",
    "                    for ss in range(0,SS_length-3):\n",
    "                        if len(Action_set_DIFT[ss]) == 0:\n",
    "                            Total_Actions_DIFT = Total_Actions_DIFT + 1\n",
    "                        else:\n",
    "                            Total_Actions_DIFT = Total_Actions_DIFT + len(Action_set_DIFT[ss])\n",
    "\n",
    "                    for ss in range(0,SS_length-3):\n",
    "                        if ss == current_state:\n",
    "                            break \n",
    "                    \n",
    "                        if len(Action_set_DIFT[ss]) == 0:\n",
    "                            start_ptr_DIFT_policy = start_ptr_DIFT_policy + 1\n",
    "                        else:\n",
    "                            start_ptr_DIFT_policy = start_ptr_DIFT_policy + len(Action_set_DIFT[ss])\n",
    "                    \n",
    "                        if len(Action_set_APT[ss]) == 0:\n",
    "                            start_ptr_APT_policy = start_ptr_APT_policy + 1\n",
    "                        else:\n",
    "                            start_ptr_APT_policy = start_ptr_APT_policy + len(Action_set_APT[ss])    \n",
    "            \n",
    "                    end_ptr_DIFT_policy = start_ptr_DIFT_policy + len(Action_set_DIFT[current_state]) \n",
    "                    start_ptr_APT_policy = start_ptr_APT_policy + Total_Actions_DIFT \n",
    "                    end_ptr_APT_policy = start_ptr_APT_policy + len(Action_set_APT[current_state]) \n",
    "            \n",
    "                    #Replacing places in random policy input with deterministic policies corresponding to the current state \n",
    "                    Input_vector[0][start_ptr_DIFT_policy:end_ptr_DIFT_policy] = PdPa_current_state[0:num_DIFT_actions]\n",
    "                    Input_vector[0][start_ptr_APT_policy:end_ptr_APT_policy] = PdPa_current_state[num_DIFT_actions:]\n",
    "                \n",
    "                    #Estimating Q values using NN trained model \n",
    "                    Value_predict = NN_model.predict(Input_vector)\n",
    "                    Q_est[dd][aa] = Value_predict[0][current_state]\n",
    "                    \n",
    "            #Solving the stage game using LP with estimated Q values if estimate = True\n",
    "            Policy_Values = Stage_Game_solver(current_state,Q_est,V,estimate = True)\n",
    "            Input_vector[0][start_ptr_DIFT_policy:end_ptr_DIFT_policy] = Policy_Values[0]\n",
    "            Input_vector[0][start_ptr_APT_policy:end_ptr_APT_policy] = Policy_Values[1]\n",
    "\n",
    "        V[current_state] = Policy_Values[2]\n",
    "        \n",
    "        \n",
    "\n",
    "    # Virtual state\n",
    "    tmp_s0_value = beta\n",
    "    for nn in range(0, len(entry_points)):\n",
    "        if tmp_s0_value > V[entry_points[nn]]:\n",
    "            tmp_s0_value = V[entry_points[nn]]\n",
    "                        \n",
    "    V[IFG_length] = tmp_s0_value\n",
    "\n",
    "    #Setting values at the intermediate destination related states\n",
    "    for ii in intermediate_destinations.tolist(): \n",
    "        neighbor = np.nonzero(IFG[ii, :])\n",
    "        V[ii] = V[neighbor[0]]\n",
    "    \n",
    "    return V    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define game related parameters here (e.g., IFG, entrypoints, destinations, rewards, etc...)\n",
    "\n",
    "#Load Ransomware attack state space (without s0, \\Tau_A, \\Tau_B and \\Phi states)\n",
    "IFG_Data = loadmat('Ransomeware_Data')\n",
    "IFG = IFG_Data['state_transition_mat']\n",
    "\n",
    "#Define set of states corresponding to entrypoint(s) of the attack \n",
    "entry_points = np.array([0, 6])\n",
    "#Define set of states corresponding to final destination(s) of the attack \n",
    "destination = np.array([16])\n",
    "#Define set of states corresponding to intermediate destinaton(s) of the attack \n",
    "intermediate_destinations = np.array([2,9])\n",
    "#Define False negatives asscoiated with the states \n",
    "FN = [0.1, 0.2, 0.05, 0.1, 0.1, 0.1, 0.2, 0.05, 0.1, 0.1, 0.1, 0.2, 0.05, 0.1, 0.1, 0.1, 0.2, 0.05, 0.1, 0.1]\n",
    "#Define False positives asscoiated with the states \n",
    "FP = [0.2, 0.1, 0.1, 0.05, 0.1, 0.2, 0.1, 0.1, 0.05, 0.1, 0.2, 0.1, 0.1, 0.05, 0.1, 0.2, 0.1, 0.1, 0.05, 0.1]\n",
    "\n",
    "#Define reward value (beta) \n",
    "beta = 50\n",
    "Tdata_len = np.array([1, 2, 4, 6, 8, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 125, 150, 175, 200, 250]).tolist()\n",
    "\n",
    "IFG_length = len(IFG)\n",
    "IFG_Graph = nx.from_numpy_matrix(np.array(IFG))\n",
    "\n",
    "IFG_graph = nx.DiGraph()\n",
    "for ii in range(IFG_length):\n",
    "    for jj in range(IFG_length):\n",
    "        if IFG[ii, jj] == 1:\n",
    "            IFG_graph.add_edge(ii, jj)\n",
    "\n",
    "\n",
    "\n",
    "SS_length = IFG_length + 4  # s0, tau_A, tau_B, tau_C\n",
    "SS = np.zeros((SS_length, SS_length))\n",
    "\n",
    "for ii in range(SS_length - 1):\n",
    "    if ii <= (IFG_length - 1) and ii != destination[0]:\n",
    "        SS[ii, IFG_length + 1:IFG_length + 4] = np.ones((1, 3), dtype=int)\n",
    "    if ii == IFG_length:\n",
    "        SS[ii, entry_points] = 1\n",
    "    for jj in range(SS_length - 1):\n",
    "        if ii <= (IFG_length - 1) and jj <= (IFG_length - 1):\n",
    "            SS[ii, jj] = IFG[ii, jj]\n",
    "\n",
    "SS_graph = nx.DiGraph()\n",
    "for ii in range(SS_length):\n",
    "    for jj in range(SS_length):\n",
    "        if SS[ii, jj] == 1:\n",
    "            SS_graph.add_edge(ii, jj)\n",
    "\n",
    "# nx.draw_networkx(IFG_graph, with_labels=True)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# Action set of players at IFG nondestination nodes and policy of defender\n",
    "# Action sets\n",
    "Action_set_DIFT = []\n",
    "Action_set_APT = []\n",
    "neighbors = []\n",
    "\n",
    "for ii in range(0, IFG_length + 1):\n",
    "    if (ii == IFG_length):  # Action sets at the virtual state\n",
    "        Action_set_DIFT.append([-2])  #action of DIFT ---> pseudo action\n",
    "        Action_set_APT.append(entry_points.tolist())\n",
    "    elif (ii in destination) or (ii in intermediate_destinations):\n",
    "        Action_set_DIFT.append([-2]) #action of DIFT ---> pseudo action\n",
    "        Action_set_APT.append([-2]) #action of APT ---> pseudo action  \n",
    "    else:\n",
    "        neighbors = np.nonzero(IFG[ii, :])\n",
    "        Action_set_DIFT.append(neighbors[0])\n",
    "        Action_set_APT.append(neighbors[0])\n",
    "        x = Action_set_DIFT[ii].tolist()\n",
    "        x.extend([-1])  # -1 action of DIFT ---> performing a security analysis\n",
    "        y = Action_set_APT[ii].tolist()\n",
    "        y.extend([-1])  # -1 action of APT ---> quitting thee attack\n",
    "        Action_set_DIFT[ii] = x\n",
    "        Action_set_APT[ii] = y\n",
    "\n",
    "\n",
    "\n",
    "#Topological order for solving stage games (Only consider IFG node related states)\n",
    "TP_Order = list(reversed(list(nx.topological_sort(IFG_graph))))\n",
    "SS_To_Visit = TP_Order.copy() #List will keep only non-destination related IFG states \n",
    "for ii in range(0,len(TP_Order)):\n",
    "    dest_flag = TP_Order[ii] in destination.tolist()\n",
    "    inter_dest_flag = TP_Order[ii] in intermediate_destinations.tolist()\n",
    "    if int(dest_flag) == 1 or int(inter_dest_flag) == 1:\n",
    "        pop_index = SS_To_Visit.index(TP_Order[ii])\n",
    "        SS_To_Visit.pop(pop_index)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TD length: 1000\n",
      "epoch: 0, train mae: 12167.0, vali mae: 12018.0, test mae: 11640.0\n",
      "epoch: 1, train mae: 35872.0, vali mae: 36675.0, test mae: 36635.0\n",
      "epoch: 2, train mae: 35630.0, vali mae: 36431.0, test mae: 36386.0\n",
      "epoch: 3, train mae: 33328.0, vali mae: 34066.0, test mae: 33971.0\n",
      "epoch: 4, train mae: 24218.0, vali mae: 24574.0, test mae: 23993.0\n",
      "epoch: 5, train mae: 31647.0, vali mae: 32253.0, test mae: 32165.0\n",
      "epoch: 6, train mae: 31354.0, vali mae: 31921.0, test mae: 31789.0\n",
      "epoch: 7, train mae: 19874.0, vali mae: 19243.0, test mae: 19167.0\n",
      "epoch: 8, train mae: 27989.0, vali mae: 28604.0, test mae: 28242.0\n",
      "epoch: 9, train mae: 24858.0, vali mae: 25387.0, test mae: 24951.0\n",
      "epoch: 10, train mae: 13581.0, vali mae: 13098.0, test mae: 12839.0\n",
      "epoch: 11, train mae: 12211.0, vali mae: 11989.0, test mae: 11669.0\n",
      "epoch: 12, train mae: 9897.0, vali mae: 9156.0, test mae: 9184.0\n",
      "epoch: 13, train mae: 9636.0, vali mae: 8929.0, test mae: 8936.0\n",
      "epoch: 14, train mae: 9656.0, vali mae: 9024.0, test mae: 8966.0\n",
      "epoch: 15, train mae: 9614.0, vali mae: 8979.0, test mae: 8921.0\n",
      "epoch: 16, train mae: 9613.0, vali mae: 8951.0, test mae: 8893.0\n",
      "epoch: 17, train mae: 9497.0, vali mae: 8890.0, test mae: 8803.0\n",
      "epoch: 18, train mae: 9557.0, vali mae: 8929.0, test mae: 8856.0\n",
      "epoch: 19, train mae: 9529.0, vali mae: 8895.0, test mae: 8825.0\n",
      "epoch: 20, train mae: 9515.0, vali mae: 8878.0, test mae: 8820.0\n",
      "epoch: 21, train mae: 9469.0, vali mae: 8839.0, test mae: 8757.0\n",
      "epoch: 22, train mae: 9479.0, vali mae: 8824.0, test mae: 8780.0\n",
      "epoch: 23, train mae: 9404.0, vali mae: 8753.0, test mae: 8692.0\n",
      "epoch: 24, train mae: 9364.0, vali mae: 8722.0, test mae: 8659.0\n",
      "epoch: 25, train mae: 9339.0, vali mae: 8685.0, test mae: 8625.0\n",
      "epoch: 26, train mae: 9391.0, vali mae: 8752.0, test mae: 8682.0\n",
      "epoch: 27, train mae: 9288.0, vali mae: 8640.0, test mae: 8585.0\n",
      "epoch: 28, train mae: 9261.0, vali mae: 8620.0, test mae: 8562.0\n",
      "epoch: 29, train mae: 9279.0, vali mae: 8643.0, test mae: 8590.0\n",
      "epoch: 30, train mae: 9272.0, vali mae: 8661.0, test mae: 8592.0\n",
      "epoch: 31, train mae: 9217.0, vali mae: 8592.0, test mae: 8538.0\n",
      "epoch: 32, train mae: 9181.0, vali mae: 8570.0, test mae: 8522.0\n",
      "epoch: 33, train mae: 9142.0, vali mae: 8540.0, test mae: 8490.0\n",
      "epoch: 34, train mae: 9128.0, vali mae: 8541.0, test mae: 8496.0\n",
      "epoch: 35, train mae: 9094.0, vali mae: 8505.0, test mae: 8470.0\n",
      "epoch: 36, train mae: 9013.0, vali mae: 8436.0, test mae: 8406.0\n",
      "epoch: 37, train mae: 9143.0, vali mae: 8578.0, test mae: 8563.0\n",
      "epoch: 38, train mae: 9102.0, vali mae: 8542.0, test mae: 8504.0\n",
      "epoch: 39, train mae: 9139.0, vali mae: 8596.0, test mae: 8557.0\n",
      "epoch: 40, train mae: 8841.0, vali mae: 8299.0, test mae: 8275.0\n",
      "epoch: 41, train mae: 8814.0, vali mae: 8281.0, test mae: 8245.0\n",
      "epoch: 42, train mae: 8800.0, vali mae: 8260.0, test mae: 8229.0\n",
      "epoch: 43, train mae: 8825.0, vali mae: 8310.0, test mae: 8294.0\n",
      "epoch: 44, train mae: 8771.0, vali mae: 8258.0, test mae: 8220.0\n",
      "epoch: 45, train mae: 8693.0, vali mae: 8213.0, test mae: 8158.0\n",
      "epoch: 46, train mae: 8626.0, vali mae: 8147.0, test mae: 8092.0\n",
      "epoch: 47, train mae: 8639.0, vali mae: 8191.0, test mae: 8113.0\n",
      "epoch: 48, train mae: 8524.0, vali mae: 8084.0, test mae: 7989.0\n",
      "epoch: 49, train mae: 8587.0, vali mae: 8131.0, test mae: 8081.0\n",
      "epoch: 50, train mae: 8519.0, vali mae: 8118.0, test mae: 8009.0\n",
      "epoch: 51, train mae: 8450.0, vali mae: 8025.0, test mae: 7939.0\n",
      "epoch: 52, train mae: 8479.0, vali mae: 8105.0, test mae: 7988.0\n",
      "epoch: 53, train mae: 8487.0, vali mae: 8093.0, test mae: 7995.0\n",
      "epoch: 54, train mae: 8495.0, vali mae: 8169.0, test mae: 8002.0\n",
      "epoch: 55, train mae: 8230.0, vali mae: 7886.0, test mae: 7769.0\n",
      "epoch: 56, train mae: 8235.0, vali mae: 7904.0, test mae: 7755.0\n",
      "epoch: 57, train mae: 8225.0, vali mae: 7953.0, test mae: 7780.0\n",
      "epoch: 58, train mae: 8187.0, vali mae: 7888.0, test mae: 7716.0\n",
      "epoch: 59, train mae: 8310.0, vali mae: 8081.0, test mae: 7873.0\n",
      "epoch: 60, train mae: 7999.0, vali mae: 7739.0, test mae: 7572.0\n",
      "epoch: 61, train mae: 7984.0, vali mae: 7737.0, test mae: 7547.0\n",
      "epoch: 62, train mae: 7966.0, vali mae: 7754.0, test mae: 7558.0\n",
      "epoch: 63, train mae: 7971.0, vali mae: 7737.0, test mae: 7556.0\n",
      "epoch: 64, train mae: 7925.0, vali mae: 7739.0, test mae: 7523.0\n",
      "epoch: 65, train mae: 7846.0, vali mae: 7651.0, test mae: 7445.0\n",
      "epoch: 66, train mae: 7850.0, vali mae: 7697.0, test mae: 7478.0\n",
      "epoch: 67, train mae: 7806.0, vali mae: 7637.0, test mae: 7410.0\n",
      "epoch: 68, train mae: 7799.0, vali mae: 7670.0, test mae: 7445.0\n",
      "epoch: 69, train mae: 7695.0, vali mae: 7576.0, test mae: 7324.0\n",
      "epoch: 70, train mae: 7716.0, vali mae: 7622.0, test mae: 7384.0\n",
      "epoch: 71, train mae: 7640.0, vali mae: 7525.0, test mae: 7269.0\n",
      "epoch: 72, train mae: 7639.0, vali mae: 7570.0, test mae: 7301.0\n",
      "epoch: 73, train mae: 7744.0, vali mae: 7619.0, test mae: 7417.0\n",
      "epoch: 74, train mae: 7616.0, vali mae: 7575.0, test mae: 7303.0\n",
      "epoch: 75, train mae: 7502.0, vali mae: 7456.0, test mae: 7187.0\n",
      "epoch: 76, train mae: 7572.0, vali mae: 7564.0, test mae: 7278.0\n",
      "epoch: 77, train mae: 7496.0, vali mae: 7467.0, test mae: 7164.0\n",
      "epoch: 78, train mae: 7458.0, vali mae: 7472.0, test mae: 7194.0\n",
      "epoch: 79, train mae: 7419.0, vali mae: 7406.0, test mae: 7121.0\n",
      "epoch: 80, train mae: 7349.0, vali mae: 7380.0, test mae: 7075.0\n",
      "epoch: 81, train mae: 7325.0, vali mae: 7356.0, test mae: 7047.0\n",
      "epoch: 82, train mae: 7308.0, vali mae: 7343.0, test mae: 7025.0\n",
      "epoch: 83, train mae: 7291.0, vali mae: 7345.0, test mae: 7018.0\n",
      "epoch: 84, train mae: 7268.0, vali mae: 7332.0, test mae: 7005.0\n",
      "epoch: 85, train mae: 7281.0, vali mae: 7364.0, test mae: 7023.0\n",
      "epoch: 86, train mae: 7232.0, vali mae: 7317.0, test mae: 6974.0\n",
      "epoch: 87, train mae: 7210.0, vali mae: 7296.0, test mae: 6959.0\n",
      "epoch: 88, train mae: 7198.0, vali mae: 7298.0, test mae: 6963.0\n",
      "epoch: 89, train mae: 7201.0, vali mae: 7284.0, test mae: 6957.0\n",
      "epoch: 90, train mae: 7160.0, vali mae: 7286.0, test mae: 6928.0\n",
      "epoch: 91, train mae: 7161.0, vali mae: 7299.0, test mae: 6943.0\n",
      "epoch: 92, train mae: 7122.0, vali mae: 7258.0, test mae: 6904.0\n",
      "epoch: 93, train mae: 7105.0, vali mae: 7258.0, test mae: 6887.0\n",
      "epoch: 94, train mae: 7101.0, vali mae: 7266.0, test mae: 6894.0\n",
      "epoch: 95, train mae: 7074.0, vali mae: 7241.0, test mae: 6862.0\n",
      "epoch: 96, train mae: 7111.0, vali mae: 7245.0, test mae: 6892.0\n",
      "epoch: 97, train mae: 7054.0, vali mae: 7232.0, test mae: 6850.0\n",
      "epoch: 98, train mae: 7036.0, vali mae: 7206.0, test mae: 6836.0\n",
      "epoch: 99, train mae: 7070.0, vali mae: 7264.0, test mae: 6886.0\n",
      "Saved-Not a good approximation!\n",
      "Exact: [48.63636364 43.18181818 40.90909091 47.5        47.5        47.5\n",
      " 48.63636364 40.90909091 45.         36.98113208 45.         45.\n",
      " 45.         45.         36.98113208 40.          0.         49.\n",
      " 49.         49.         48.63636364 50.          0.         50.        ]\n",
      "Estimated sample: [45.90926361 32.3674736  24.36826324 40.81404114 35.86131287 36.21639252\n",
      " 45.47259903 24.36826324 23.10817909 33.55573654 29.50423813 28.28120041\n",
      " 34.45117188 26.87413597 33.55573654 25.22895622  0.         48.89737701\n",
      " 48.67047882 48.08334732 45.47259903 50.          0.         50.        ]\n",
      "TD length: 2000\n",
      "epoch: 0, train mae: 36280.0, vali mae: 36263.0, test mae: 36978.0\n",
      "epoch: 1, train mae: 35371.0, vali mae: 35354.0, test mae: 36080.0\n",
      "epoch: 2, train mae: 11349.0, vali mae: 11339.0, test mae: 11042.0\n",
      "epoch: 3, train mae: 12933.0, vali mae: 12902.0, test mae: 12510.0\n",
      "epoch: 4, train mae: 13919.0, vali mae: 13715.0, test mae: 13434.0\n",
      "epoch: 5, train mae: 22785.0, vali mae: 22762.0, test mae: 23347.0\n",
      "epoch: 6, train mae: 15927.0, vali mae: 15837.0, test mae: 15340.0\n",
      "epoch: 7, train mae: 9342.0, vali mae: 9296.0, test mae: 8684.0\n",
      "epoch: 8, train mae: 9369.0, vali mae: 9325.0, test mae: 8756.0\n",
      "epoch: 9, train mae: 9249.0, vali mae: 9199.0, test mae: 8620.0\n",
      "epoch: 10, train mae: 9201.0, vali mae: 9153.0, test mae: 8574.0\n",
      "epoch: 11, train mae: 9095.0, vali mae: 9049.0, test mae: 8469.0\n",
      "epoch: 12, train mae: 9161.0, vali mae: 9115.0, test mae: 8535.0\n",
      "epoch: 13, train mae: 9200.0, vali mae: 9180.0, test mae: 8608.0\n",
      "epoch: 14, train mae: 9002.0, vali mae: 8979.0, test mae: 8403.0\n",
      "epoch: 15, train mae: 8933.0, vali mae: 8905.0, test mae: 8373.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16, train mae: 8955.0, vali mae: 8922.0, test mae: 8402.0\n",
      "epoch: 17, train mae: 9021.0, vali mae: 8990.0, test mae: 8479.0\n",
      "epoch: 18, train mae: 8733.0, vali mae: 8700.0, test mae: 8211.0\n",
      "epoch: 19, train mae: 8792.0, vali mae: 8744.0, test mae: 8268.0\n",
      "epoch: 20, train mae: 8662.0, vali mae: 8634.0, test mae: 8169.0\n",
      "epoch: 21, train mae: 8590.0, vali mae: 8543.0, test mae: 8097.0\n",
      "epoch: 22, train mae: 8505.0, vali mae: 8467.0, test mae: 8017.0\n",
      "epoch: 23, train mae: 8749.0, vali mae: 8719.0, test mae: 8321.0\n",
      "epoch: 24, train mae: 8333.0, vali mae: 8305.0, test mae: 7880.0\n",
      "epoch: 25, train mae: 8263.0, vali mae: 8248.0, test mae: 7844.0\n",
      "epoch: 26, train mae: 8216.0, vali mae: 8194.0, test mae: 7807.0\n",
      "epoch: 27, train mae: 8089.0, vali mae: 8088.0, test mae: 7706.0\n",
      "epoch: 28, train mae: 8072.0, vali mae: 8079.0, test mae: 7691.0\n",
      "epoch: 29, train mae: 8043.0, vali mae: 8104.0, test mae: 7734.0\n",
      "epoch: 30, train mae: 7736.0, vali mae: 7799.0, test mae: 7416.0\n",
      "epoch: 31, train mae: 7849.0, vali mae: 7944.0, test mae: 7592.0\n",
      "epoch: 32, train mae: 7465.0, vali mae: 7566.0, test mae: 7232.0\n",
      "epoch: 33, train mae: 7395.0, vali mae: 7478.0, test mae: 7184.0\n",
      "epoch: 34, train mae: 7308.0, vali mae: 7430.0, test mae: 7143.0\n",
      "epoch: 35, train mae: 7396.0, vali mae: 7570.0, test mae: 7286.0\n",
      "epoch: 36, train mae: 7103.0, vali mae: 7215.0, test mae: 6967.0\n",
      "epoch: 37, train mae: 6871.0, vali mae: 7061.0, test mae: 6803.0\n",
      "epoch: 38, train mae: 6561.0, vali mae: 6770.0, test mae: 6523.0\n",
      "epoch: 39, train mae: 6601.0, vali mae: 6833.0, test mae: 6599.0\n",
      "epoch: 40, train mae: 6377.0, vali mae: 6572.0, test mae: 6394.0\n",
      "epoch: 41, train mae: 6167.0, vali mae: 6430.0, test mae: 6200.0\n",
      "epoch: 42, train mae: 6044.0, vali mae: 6297.0, test mae: 6092.0\n",
      "epoch: 43, train mae: 5982.0, vali mae: 6276.0, test mae: 6028.0\n",
      "epoch: 44, train mae: 5784.0, vali mae: 6039.0, test mae: 5820.0\n",
      "epoch: 45, train mae: 5586.0, vali mae: 5892.0, test mae: 5633.0\n",
      "epoch: 46, train mae: 5475.0, vali mae: 5775.0, test mae: 5535.0\n",
      "epoch: 47, train mae: 5305.0, vali mae: 5617.0, test mae: 5337.0\n",
      "epoch: 48, train mae: 5238.0, vali mae: 5531.0, test mae: 5271.0\n",
      "epoch: 49, train mae: 4998.0, vali mae: 5307.0, test mae: 5014.0\n",
      "epoch: 50, train mae: 4927.0, vali mae: 5245.0, test mae: 4940.0\n",
      "epoch: 51, train mae: 4722.0, vali mae: 5043.0, test mae: 4731.0\n",
      "epoch: 52, train mae: 4686.0, vali mae: 5026.0, test mae: 4691.0\n",
      "epoch: 53, train mae: 4543.0, vali mae: 4857.0, test mae: 4521.0\n",
      "epoch: 54, train mae: 4484.0, vali mae: 4822.0, test mae: 4472.0\n",
      "epoch: 55, train mae: 4271.0, vali mae: 4595.0, test mae: 4251.0\n",
      "epoch: 56, train mae: 4162.0, vali mae: 4497.0, test mae: 4120.0\n",
      "epoch: 57, train mae: 4174.0, vali mae: 4512.0, test mae: 4136.0\n",
      "epoch: 58, train mae: 4093.0, vali mae: 4379.0, test mae: 4011.0\n",
      "epoch: 59, train mae: 4044.0, vali mae: 4374.0, test mae: 3963.0\n",
      "epoch: 60, train mae: 3842.0, vali mae: 4179.0, test mae: 3795.0\n",
      "epoch: 61, train mae: 3785.0, vali mae: 4119.0, test mae: 3723.0\n",
      "epoch: 62, train mae: 3763.0, vali mae: 4079.0, test mae: 3704.0\n",
      "epoch: 63, train mae: 3748.0, vali mae: 4059.0, test mae: 3687.0\n",
      "epoch: 64, train mae: 3693.0, vali mae: 4016.0, test mae: 3619.0\n",
      "epoch: 65, train mae: 3642.0, vali mae: 3954.0, test mae: 3595.0\n",
      "epoch: 66, train mae: 3552.0, vali mae: 3873.0, test mae: 3481.0\n",
      "epoch: 67, train mae: 3546.0, vali mae: 3849.0, test mae: 3472.0\n",
      "epoch: 68, train mae: 3527.0, vali mae: 3840.0, test mae: 3436.0\n",
      "epoch: 69, train mae: 3418.0, vali mae: 3730.0, test mae: 3360.0\n",
      "epoch: 70, train mae: 3447.0, vali mae: 3753.0, test mae: 3373.0\n",
      "epoch: 71, train mae: 3351.0, vali mae: 3655.0, test mae: 3285.0\n",
      "epoch: 72, train mae: 3302.0, vali mae: 3617.0, test mae: 3246.0\n",
      "epoch: 73, train mae: 3309.0, vali mae: 3607.0, test mae: 3267.0\n",
      "epoch: 74, train mae: 3252.0, vali mae: 3565.0, test mae: 3183.0\n",
      "epoch: 75, train mae: 3169.0, vali mae: 3475.0, test mae: 3136.0\n",
      "epoch: 76, train mae: 3158.0, vali mae: 3463.0, test mae: 3097.0\n",
      "epoch: 77, train mae: 3105.0, vali mae: 3401.0, test mae: 3070.0\n",
      "epoch: 78, train mae: 3118.0, vali mae: 3421.0, test mae: 3062.0\n",
      "epoch: 79, train mae: 3092.0, vali mae: 3387.0, test mae: 3064.0\n",
      "epoch: 80, train mae: 3007.0, vali mae: 3313.0, test mae: 2981.0\n",
      "epoch: 81, train mae: 2986.0, vali mae: 3296.0, test mae: 2956.0\n",
      "epoch: 82, train mae: 2968.0, vali mae: 3273.0, test mae: 2942.0\n",
      "epoch: 83, train mae: 2956.0, vali mae: 3262.0, test mae: 2932.0\n",
      "epoch: 84, train mae: 2946.0, vali mae: 3251.0, test mae: 2929.0\n",
      "epoch: 85, train mae: 2943.0, vali mae: 3243.0, test mae: 2916.0\n",
      "epoch: 86, train mae: 2910.0, vali mae: 3212.0, test mae: 2888.0\n",
      "epoch: 87, train mae: 2896.0, vali mae: 3196.0, test mae: 2873.0\n",
      "epoch: 88, train mae: 2904.0, vali mae: 3200.0, test mae: 2869.0\n",
      "epoch: 89, train mae: 2867.0, vali mae: 3168.0, test mae: 2844.0\n",
      "epoch: 90, train mae: 2855.0, vali mae: 3154.0, test mae: 2837.0\n",
      "epoch: 91, train mae: 2851.0, vali mae: 3148.0, test mae: 2833.0\n",
      "epoch: 92, train mae: 2859.0, vali mae: 3155.0, test mae: 2827.0\n",
      "epoch: 93, train mae: 2835.0, vali mae: 3131.0, test mae: 2809.0\n",
      "epoch: 94, train mae: 2829.0, vali mae: 3123.0, test mae: 2807.0\n",
      "epoch: 95, train mae: 2820.0, vali mae: 3111.0, test mae: 2784.0\n",
      "epoch: 96, train mae: 2796.0, vali mae: 3091.0, test mae: 2777.0\n",
      "epoch: 97, train mae: 2793.0, vali mae: 3082.0, test mae: 2752.0\n",
      "epoch: 98, train mae: 2775.0, vali mae: 3072.0, test mae: 2752.0\n",
      "epoch: 99, train mae: 2815.0, vali mae: 3095.0, test mae: 2770.0\n",
      "Saved-Not a good approximation!\n",
      "Exact: [48.63636364 43.18181818 40.90909091 47.5        47.5        47.5\n",
      " 48.63636364 40.90909091 45.         36.98113208 45.         45.\n",
      " 45.         45.         36.98113208 40.          0.         49.\n",
      " 49.         49.         48.63636364 50.          0.         50.        ]\n",
      "Estimated sample: [43.77089691 19.06566811  5.64159203 46.84165955 46.34780121 48.09886551\n",
      " 44.8685112   5.64159203 42.88063049  1.01662946 40.05199051 44.2438736\n",
      " 44.91065216 43.33521271  1.01662946 39.61539078  0.         48.71731949\n",
      " 48.4926796  48.41070175 43.77089691 50.          0.         50.        ]\n",
      "TD length: 4000\n",
      "epoch: 0, train mae: 35080.0, vali mae: 34810.0, test mae: 35257.0\n",
      "epoch: 1, train mae: 29423.0, vali mae: 29211.0, test mae: 29584.0\n",
      "epoch: 2, train mae: 9906.0, vali mae: 10170.0, test mae: 9774.0\n",
      "epoch: 3, train mae: 9184.0, vali mae: 9457.0, test mae: 9052.0\n",
      "epoch: 4, train mae: 9033.0, vali mae: 9315.0, test mae: 8912.0\n",
      "epoch: 5, train mae: 8835.0, vali mae: 9140.0, test mae: 8724.0\n",
      "epoch: 6, train mae: 8801.0, vali mae: 9081.0, test mae: 8695.0\n",
      "epoch: 7, train mae: 8591.0, vali mae: 8880.0, test mae: 8486.0\n",
      "epoch: 8, train mae: 8689.0, vali mae: 8974.0, test mae: 8606.0\n",
      "epoch: 9, train mae: 8343.0, vali mae: 8639.0, test mae: 8275.0\n",
      "epoch: 10, train mae: 7939.0, vali mae: 8218.0, test mae: 7880.0\n",
      "epoch: 11, train mae: 7788.0, vali mae: 8059.0, test mae: 7770.0\n",
      "epoch: 12, train mae: 7616.0, vali mae: 7898.0, test mae: 7583.0\n",
      "epoch: 13, train mae: 7672.0, vali mae: 7927.0, test mae: 7744.0\n",
      "epoch: 14, train mae: 6602.0, vali mae: 6906.0, test mae: 6657.0\n",
      "epoch: 15, train mae: 6406.0, vali mae: 6649.0, test mae: 6466.0\n",
      "epoch: 16, train mae: 5842.0, vali mae: 6100.0, test mae: 5818.0\n",
      "epoch: 17, train mae: 5512.0, vali mae: 5681.0, test mae: 5549.0\n",
      "epoch: 18, train mae: 4865.0, vali mae: 5031.0, test mae: 4865.0\n",
      "epoch: 19, train mae: 4657.0, vali mae: 4678.0, test mae: 4705.0\n",
      "epoch: 20, train mae: 4438.0, vali mae: 4551.0, test mae: 4392.0\n",
      "epoch: 21, train mae: 3874.0, vali mae: 3977.0, test mae: 3879.0\n",
      "epoch: 22, train mae: 3998.0, vali mae: 4007.0, test mae: 4021.0\n",
      "epoch: 23, train mae: 3690.0, vali mae: 3744.0, test mae: 3722.0\n",
      "epoch: 24, train mae: 3877.0, vali mae: 4007.0, test mae: 3888.0\n",
      "epoch: 25, train mae: 3519.0, vali mae: 3611.0, test mae: 3603.0\n",
      "epoch: 26, train mae: 3323.0, vali mae: 3463.0, test mae: 3376.0\n",
      "epoch: 27, train mae: 3372.0, vali mae: 3499.0, test mae: 3496.0\n",
      "epoch: 28, train mae: 3152.0, vali mae: 3280.0, test mae: 3246.0\n",
      "epoch: 29, train mae: 3081.0, vali mae: 3217.0, test mae: 3232.0\n",
      "epoch: 30, train mae: 3088.0, vali mae: 3282.0, test mae: 3227.0\n",
      "epoch: 31, train mae: 3019.0, vali mae: 3196.0, test mae: 3184.0\n",
      "epoch: 32, train mae: 2805.0, vali mae: 3007.0, test mae: 2966.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 33, train mae: 2900.0, vali mae: 3085.0, test mae: 3090.0\n",
      "epoch: 34, train mae: 2697.0, vali mae: 2916.0, test mae: 2887.0\n",
      "epoch: 35, train mae: 2816.0, vali mae: 3007.0, test mae: 3003.0\n",
      "epoch: 36, train mae: 2751.0, vali mae: 2966.0, test mae: 2909.0\n",
      "epoch: 37, train mae: 2603.0, vali mae: 2804.0, test mae: 2771.0\n",
      "epoch: 38, train mae: 2543.0, vali mae: 2757.0, test mae: 2701.0\n",
      "epoch: 39, train mae: 2606.0, vali mae: 2804.0, test mae: 2773.0\n",
      "epoch: 40, train mae: 2589.0, vali mae: 2819.0, test mae: 2756.0\n",
      "epoch: 41, train mae: 2447.0, vali mae: 2644.0, test mae: 2626.0\n",
      "epoch: 42, train mae: 2427.0, vali mae: 2664.0, test mae: 2603.0\n",
      "epoch: 43, train mae: 2330.0, vali mae: 2522.0, test mae: 2493.0\n",
      "epoch: 44, train mae: 2236.0, vali mae: 2460.0, test mae: 2413.0\n",
      "epoch: 45, train mae: 2349.0, vali mae: 2575.0, test mae: 2496.0\n",
      "epoch: 46, train mae: 2247.0, vali mae: 2451.0, test mae: 2416.0\n",
      "epoch: 47, train mae: 2246.0, vali mae: 2464.0, test mae: 2404.0\n",
      "epoch: 48, train mae: 2308.0, vali mae: 2507.0, test mae: 2457.0\n",
      "epoch: 49, train mae: 2102.0, vali mae: 2327.0, test mae: 2275.0\n",
      "epoch: 50, train mae: 2147.0, vali mae: 2362.0, test mae: 2313.0\n",
      "epoch: 51, train mae: 2125.0, vali mae: 2359.0, test mae: 2287.0\n",
      "epoch: 52, train mae: 2085.0, vali mae: 2307.0, test mae: 2250.0\n",
      "epoch: 53, train mae: 2077.0, vali mae: 2308.0, test mae: 2242.0\n",
      "epoch: 54, train mae: 1970.0, vali mae: 2194.0, test mae: 2139.0\n",
      "epoch: 55, train mae: 2020.0, vali mae: 2253.0, test mae: 2191.0\n",
      "epoch: 56, train mae: 1959.0, vali mae: 2183.0, test mae: 2130.0\n",
      "epoch: 57, train mae: 1955.0, vali mae: 2203.0, test mae: 2118.0\n",
      "epoch: 58, train mae: 1977.0, vali mae: 2196.0, test mae: 2133.0\n",
      "epoch: 59, train mae: 1923.0, vali mae: 2164.0, test mae: 2084.0\n",
      "epoch: 60, train mae: 1950.0, vali mae: 2172.0, test mae: 2103.0\n",
      "epoch: 61, train mae: 1881.0, vali mae: 2113.0, test mae: 2046.0\n",
      "epoch: 62, train mae: 1840.0, vali mae: 2072.0, test mae: 2011.0\n",
      "epoch: 63, train mae: 1858.0, vali mae: 2101.0, test mae: 2033.0\n",
      "epoch: 64, train mae: 1835.0, vali mae: 2063.0, test mae: 1998.0\n",
      "epoch: 65, train mae: 1837.0, vali mae: 2078.0, test mae: 2003.0\n",
      "epoch: 66, train mae: 1774.0, vali mae: 2016.0, test mae: 1950.0\n",
      "epoch: 67, train mae: 1796.0, vali mae: 2026.0, test mae: 1958.0\n",
      "epoch: 68, train mae: 1762.0, vali mae: 1997.0, test mae: 1928.0\n",
      "epoch: 69, train mae: 1763.0, vali mae: 2002.0, test mae: 1932.0\n",
      "epoch: 70, train mae: 1772.0, vali mae: 1998.0, test mae: 1936.0\n",
      "epoch: 71, train mae: 1734.0, vali mae: 1970.0, test mae: 1907.0\n",
      "epoch: 72, train mae: 1726.0, vali mae: 1960.0, test mae: 1897.0\n",
      "epoch: 73, train mae: 1717.0, vali mae: 1955.0, test mae: 1889.0\n",
      "epoch: 74, train mae: 1704.0, vali mae: 1942.0, test mae: 1877.0\n",
      "epoch: 75, train mae: 1708.0, vali mae: 1943.0, test mae: 1875.0\n",
      "epoch: 76, train mae: 1704.0, vali mae: 1942.0, test mae: 1870.0\n",
      "epoch: 77, train mae: 1675.0, vali mae: 1915.0, test mae: 1844.0\n",
      "epoch: 78, train mae: 1682.0, vali mae: 1913.0, test mae: 1839.0\n",
      "epoch: 79, train mae: 1668.0, vali mae: 1904.0, test mae: 1838.0\n",
      "epoch: 80, train mae: 1665.0, vali mae: 1900.0, test mae: 1835.0\n",
      "epoch: 81, train mae: 1647.0, vali mae: 1887.0, test mae: 1811.0\n",
      "epoch: 82, train mae: 1644.0, vali mae: 1882.0, test mae: 1811.0\n",
      "epoch: 83, train mae: 1637.0, vali mae: 1872.0, test mae: 1805.0\n",
      "epoch: 84, train mae: 1628.0, vali mae: 1871.0, test mae: 1801.0\n",
      "epoch: 85, train mae: 1631.0, vali mae: 1873.0, test mae: 1798.0\n",
      "epoch: 86, train mae: 1615.0, vali mae: 1859.0, test mae: 1782.0\n",
      "epoch: 87, train mae: 1611.0, vali mae: 1851.0, test mae: 1782.0\n",
      "epoch: 88, train mae: 1609.0, vali mae: 1850.0, test mae: 1777.0\n",
      "epoch: 89, train mae: 1601.0, vali mae: 1841.0, test mae: 1773.0\n",
      "epoch: 90, train mae: 1599.0, vali mae: 1841.0, test mae: 1766.0\n",
      "epoch: 91, train mae: 1599.0, vali mae: 1839.0, test mae: 1764.0\n",
      "epoch: 92, train mae: 1586.0, vali mae: 1832.0, test mae: 1754.0\n",
      "epoch: 93, train mae: 1586.0, vali mae: 1831.0, test mae: 1751.0\n",
      "epoch: 94, train mae: 1586.0, vali mae: 1824.0, test mae: 1752.0\n",
      "epoch: 95, train mae: 1580.0, vali mae: 1821.0, test mae: 1746.0\n",
      "epoch: 96, train mae: 1572.0, vali mae: 1817.0, test mae: 1742.0\n",
      "epoch: 97, train mae: 1569.0, vali mae: 1817.0, test mae: 1736.0\n",
      "epoch: 98, train mae: 1565.0, vali mae: 1809.0, test mae: 1732.0\n",
      "epoch: 99, train mae: 1565.0, vali mae: 1807.0, test mae: 1734.0\n",
      "Saved-Not a good approximation!\n",
      "Exact: [48.63636364 43.18181818 40.90909091 47.5        47.5        47.5\n",
      " 48.63636364 40.90909091 45.         36.98113208 45.         45.\n",
      " 45.         45.         36.98113208 40.          0.         49.\n",
      " 49.         49.         48.63636364 50.          0.         50.        ]\n",
      "Estimated sample: [44.85027313 38.42285538 38.82702413 47.73467636 48.11564255 47.63382339\n",
      " 43.63775253 38.82702413 44.85109711 37.19096222 45.27535629 45.54615402\n",
      " 44.66407776 45.57707596 37.19096222 40.5569458   0.         44.32744217\n",
      " 44.74481201 46.53185272 43.63775253 50.          0.         50.        ]\n",
      "TD length: 6000\n",
      "epoch: 0, train mae: 27761.0, vali mae: 27612.0, test mae: 27695.0\n",
      "epoch: 1, train mae: 27656.0, vali mae: 27481.0, test mae: 27561.0\n",
      "epoch: 2, train mae: 9398.0, vali mae: 9712.0, test mae: 9398.0\n",
      "epoch: 3, train mae: 9209.0, vali mae: 9512.0, test mae: 9221.0\n",
      "epoch: 4, train mae: 9188.0, vali mae: 9506.0, test mae: 9185.0\n",
      "epoch: 5, train mae: 9010.0, vali mae: 9328.0, test mae: 9015.0\n",
      "epoch: 6, train mae: 9050.0, vali mae: 9375.0, test mae: 9058.0\n",
      "epoch: 7, train mae: 8700.0, vali mae: 9024.0, test mae: 8703.0\n",
      "epoch: 8, train mae: 8563.0, vali mae: 8859.0, test mae: 8586.0\n",
      "epoch: 9, train mae: 8114.0, vali mae: 8397.0, test mae: 8133.0\n",
      "epoch: 10, train mae: 7566.0, vali mae: 7838.0, test mae: 7581.0\n",
      "epoch: 11, train mae: 6804.0, vali mae: 7066.0, test mae: 6796.0\n",
      "epoch: 12, train mae: 6116.0, vali mae: 6381.0, test mae: 6146.0\n",
      "epoch: 13, train mae: 5363.0, vali mae: 5599.0, test mae: 5407.0\n",
      "epoch: 14, train mae: 4722.0, vali mae: 4929.0, test mae: 4775.0\n",
      "epoch: 15, train mae: 4599.0, vali mae: 4798.0, test mae: 4676.0\n",
      "epoch: 16, train mae: 4645.0, vali mae: 4865.0, test mae: 4706.0\n",
      "epoch: 17, train mae: 4082.0, vali mae: 4286.0, test mae: 4177.0\n",
      "epoch: 18, train mae: 4375.0, vali mae: 4635.0, test mae: 4464.0\n",
      "epoch: 19, train mae: 4071.0, vali mae: 4314.0, test mae: 4165.0\n",
      "epoch: 20, train mae: 4145.0, vali mae: 4397.0, test mae: 4243.0\n",
      "epoch: 21, train mae: 3757.0, vali mae: 3993.0, test mae: 3870.0\n",
      "epoch: 22, train mae: 3522.0, vali mae: 3748.0, test mae: 3609.0\n",
      "epoch: 23, train mae: 3460.0, vali mae: 3715.0, test mae: 3592.0\n",
      "epoch: 24, train mae: 3128.0, vali mae: 3355.0, test mae: 3254.0\n",
      "epoch: 25, train mae: 3109.0, vali mae: 3338.0, test mae: 3241.0\n",
      "epoch: 26, train mae: 3013.0, vali mae: 3248.0, test mae: 3154.0\n",
      "epoch: 27, train mae: 3013.0, vali mae: 3240.0, test mae: 3152.0\n",
      "epoch: 28, train mae: 2827.0, vali mae: 3065.0, test mae: 2982.0\n",
      "epoch: 29, train mae: 2714.0, vali mae: 2972.0, test mae: 2893.0\n",
      "epoch: 30, train mae: 2759.0, vali mae: 3001.0, test mae: 2926.0\n",
      "epoch: 31, train mae: 2584.0, vali mae: 2830.0, test mae: 2770.0\n",
      "epoch: 32, train mae: 2587.0, vali mae: 2855.0, test mae: 2771.0\n",
      "epoch: 33, train mae: 2464.0, vali mae: 2715.0, test mae: 2654.0\n",
      "epoch: 34, train mae: 2387.0, vali mae: 2640.0, test mae: 2576.0\n",
      "epoch: 35, train mae: 2404.0, vali mae: 2671.0, test mae: 2613.0\n",
      "epoch: 36, train mae: 2337.0, vali mae: 2584.0, test mae: 2527.0\n",
      "epoch: 37, train mae: 2343.0, vali mae: 2586.0, test mae: 2530.0\n",
      "epoch: 38, train mae: 2286.0, vali mae: 2557.0, test mae: 2488.0\n",
      "epoch: 39, train mae: 2165.0, vali mae: 2436.0, test mae: 2371.0\n",
      "epoch: 40, train mae: 2274.0, vali mae: 2554.0, test mae: 2460.0\n",
      "epoch: 41, train mae: 2337.0, vali mae: 2583.0, test mae: 2519.0\n",
      "epoch: 42, train mae: 2081.0, vali mae: 2342.0, test mae: 2283.0\n",
      "epoch: 43, train mae: 1938.0, vali mae: 2220.0, test mae: 2142.0\n",
      "epoch: 44, train mae: 2041.0, vali mae: 2296.0, test mae: 2232.0\n",
      "epoch: 45, train mae: 1943.0, vali mae: 2216.0, test mae: 2119.0\n",
      "epoch: 46, train mae: 1803.0, vali mae: 2054.0, test mae: 2000.0\n",
      "epoch: 47, train mae: 1875.0, vali mae: 2128.0, test mae: 2060.0\n",
      "epoch: 48, train mae: 1868.0, vali mae: 2110.0, test mae: 2057.0\n",
      "epoch: 49, train mae: 1755.0, vali mae: 2011.0, test mae: 1946.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 50, train mae: 1712.0, vali mae: 1980.0, test mae: 1903.0\n",
      "epoch: 51, train mae: 1673.0, vali mae: 1946.0, test mae: 1870.0\n",
      "epoch: 52, train mae: 1648.0, vali mae: 1911.0, test mae: 1841.0\n",
      "epoch: 53, train mae: 1597.0, vali mae: 1868.0, test mae: 1785.0\n",
      "epoch: 54, train mae: 1645.0, vali mae: 1894.0, test mae: 1837.0\n",
      "epoch: 55, train mae: 1552.0, vali mae: 1811.0, test mae: 1738.0\n",
      "epoch: 56, train mae: 1531.0, vali mae: 1798.0, test mae: 1727.0\n",
      "epoch: 57, train mae: 1532.0, vali mae: 1800.0, test mae: 1723.0\n",
      "epoch: 58, train mae: 1483.0, vali mae: 1751.0, test mae: 1684.0\n",
      "epoch: 59, train mae: 1503.0, vali mae: 1764.0, test mae: 1698.0\n",
      "epoch: 60, train mae: 1561.0, vali mae: 1816.0, test mae: 1744.0\n",
      "epoch: 61, train mae: 1483.0, vali mae: 1745.0, test mae: 1671.0\n",
      "epoch: 62, train mae: 1447.0, vali mae: 1713.0, test mae: 1636.0\n",
      "epoch: 63, train mae: 1483.0, vali mae: 1725.0, test mae: 1668.0\n",
      "epoch: 64, train mae: 1411.0, vali mae: 1675.0, test mae: 1600.0\n",
      "epoch: 65, train mae: 1374.0, vali mae: 1644.0, test mae: 1569.0\n",
      "epoch: 66, train mae: 1365.0, vali mae: 1635.0, test mae: 1562.0\n",
      "epoch: 67, train mae: 1344.0, vali mae: 1607.0, test mae: 1536.0\n",
      "epoch: 68, train mae: 1320.0, vali mae: 1591.0, test mae: 1509.0\n",
      "epoch: 69, train mae: 1339.0, vali mae: 1600.0, test mae: 1520.0\n",
      "epoch: 70, train mae: 1283.0, vali mae: 1548.0, test mae: 1469.0\n",
      "epoch: 71, train mae: 1277.0, vali mae: 1542.0, test mae: 1464.0\n",
      "epoch: 72, train mae: 1318.0, vali mae: 1568.0, test mae: 1496.0\n",
      "epoch: 73, train mae: 1268.0, vali mae: 1531.0, test mae: 1454.0\n",
      "epoch: 74, train mae: 1260.0, vali mae: 1520.0, test mae: 1442.0\n",
      "epoch: 75, train mae: 1240.0, vali mae: 1508.0, test mae: 1428.0\n",
      "epoch: 76, train mae: 1229.0, vali mae: 1493.0, test mae: 1418.0\n",
      "epoch: 77, train mae: 1233.0, vali mae: 1493.0, test mae: 1409.0\n",
      "epoch: 78, train mae: 1218.0, vali mae: 1476.0, test mae: 1407.0\n",
      "epoch: 79, train mae: 1201.0, vali mae: 1461.0, test mae: 1394.0\n",
      "epoch: 80, train mae: 1218.0, vali mae: 1472.0, test mae: 1396.0\n",
      "epoch: 81, train mae: 1187.0, vali mae: 1442.0, test mae: 1372.0\n",
      "epoch: 82, train mae: 1204.0, vali mae: 1450.0, test mae: 1367.0\n",
      "epoch: 83, train mae: 1179.0, vali mae: 1437.0, test mae: 1356.0\n",
      "epoch: 84, train mae: 1173.0, vali mae: 1424.0, test mae: 1354.0\n",
      "epoch: 85, train mae: 1177.0, vali mae: 1426.0, test mae: 1353.0\n",
      "epoch: 86, train mae: 1159.0, vali mae: 1417.0, test mae: 1340.0\n",
      "epoch: 87, train mae: 1136.0, vali mae: 1387.0, test mae: 1317.0\n",
      "epoch: 88, train mae: 1143.0, vali mae: 1393.0, test mae: 1329.0\n",
      "epoch: 89, train mae: 1129.0, vali mae: 1382.0, test mae: 1313.0\n",
      "epoch: 90, train mae: 1120.0, vali mae: 1377.0, test mae: 1306.0\n",
      "epoch: 91, train mae: 1115.0, vali mae: 1368.0, test mae: 1302.0\n",
      "epoch: 92, train mae: 1121.0, vali mae: 1373.0, test mae: 1303.0\n",
      "epoch: 93, train mae: 1100.0, vali mae: 1357.0, test mae: 1286.0\n",
      "epoch: 94, train mae: 1102.0, vali mae: 1355.0, test mae: 1291.0\n",
      "epoch: 95, train mae: 1091.0, vali mae: 1348.0, test mae: 1278.0\n",
      "epoch: 96, train mae: 1087.0, vali mae: 1342.0, test mae: 1275.0\n",
      "epoch: 97, train mae: 1081.0, vali mae: 1334.0, test mae: 1269.0\n",
      "epoch: 98, train mae: 1081.0, vali mae: 1335.0, test mae: 1271.0\n",
      "epoch: 99, train mae: 1091.0, vali mae: 1338.0, test mae: 1275.0\n",
      "Saved-Not a good approximation!\n",
      "Exact: [48.63636364 43.18181818 40.90909091 47.5        47.5        47.5\n",
      " 48.63636364 40.90909091 45.         36.98113208 45.         45.\n",
      " 45.         45.         36.98113208 40.          0.         49.\n",
      " 49.         49.         48.63636364 50.          0.         50.        ]\n",
      "Estimated sample: [47.37670898 37.6630353  38.71120133 47.60221863 47.59909058 48.04042816\n",
      " 42.90703583 38.71120133 44.53524399 37.16385123 43.68649673 44.69553757\n",
      " 45.01576233 45.11192322 37.16385123 40.21993637  0.         48.31251526\n",
      " 46.0188446  49.35173416 42.90703583 50.          0.         50.        ]\n",
      "TD length: 8000\n",
      "epoch: 0, train mae: 30505.0, vali mae: 30385.0, test mae: 30455.0\n",
      "epoch: 1, train mae: 9315.0, vali mae: 9345.0, test mae: 9376.0\n",
      "epoch: 2, train mae: 9052.0, vali mae: 9080.0, test mae: 9102.0\n",
      "epoch: 3, train mae: 8866.0, vali mae: 8884.0, test mae: 8923.0\n",
      "epoch: 4, train mae: 8687.0, vali mae: 8720.0, test mae: 8757.0\n",
      "epoch: 5, train mae: 8127.0, vali mae: 8197.0, test mae: 8195.0\n",
      "epoch: 6, train mae: 8016.0, vali mae: 8033.0, test mae: 8169.0\n",
      "epoch: 7, train mae: 6393.0, vali mae: 6463.0, test mae: 6465.0\n",
      "epoch: 8, train mae: 5273.0, vali mae: 5397.0, test mae: 5312.0\n",
      "epoch: 9, train mae: 4565.0, vali mae: 4678.0, test mae: 4606.0\n",
      "epoch: 10, train mae: 4128.0, vali mae: 4219.0, test mae: 4193.0\n",
      "epoch: 11, train mae: 3830.0, vali mae: 3921.0, test mae: 3920.0\n",
      "epoch: 12, train mae: 3647.0, vali mae: 3740.0, test mae: 3736.0\n",
      "epoch: 13, train mae: 3450.0, vali mae: 3525.0, test mae: 3569.0\n",
      "epoch: 14, train mae: 3277.0, vali mae: 3349.0, test mae: 3412.0\n",
      "epoch: 15, train mae: 3163.0, vali mae: 3226.0, test mae: 3317.0\n",
      "epoch: 16, train mae: 3019.0, vali mae: 3073.0, test mae: 3177.0\n",
      "epoch: 17, train mae: 2809.0, vali mae: 2883.0, test mae: 2971.0\n",
      "epoch: 18, train mae: 2726.0, vali mae: 2795.0, test mae: 2885.0\n",
      "epoch: 19, train mae: 3002.0, vali mae: 3104.0, test mae: 3158.0\n",
      "epoch: 20, train mae: 2928.0, vali mae: 3017.0, test mae: 3098.0\n",
      "epoch: 21, train mae: 2378.0, vali mae: 2466.0, test mae: 2529.0\n",
      "epoch: 22, train mae: 2278.0, vali mae: 2363.0, test mae: 2418.0\n",
      "epoch: 23, train mae: 2182.0, vali mae: 2271.0, test mae: 2324.0\n",
      "epoch: 24, train mae: 2042.0, vali mae: 2124.0, test mae: 2184.0\n",
      "epoch: 25, train mae: 2126.0, vali mae: 2210.0, test mae: 2251.0\n",
      "epoch: 26, train mae: 1925.0, vali mae: 2016.0, test mae: 2060.0\n",
      "epoch: 27, train mae: 1994.0, vali mae: 2085.0, test mae: 2126.0\n",
      "epoch: 28, train mae: 1968.0, vali mae: 2049.0, test mae: 2097.0\n",
      "epoch: 29, train mae: 1733.0, vali mae: 1830.0, test mae: 1887.0\n",
      "epoch: 30, train mae: 1633.0, vali mae: 1725.0, test mae: 1777.0\n",
      "epoch: 31, train mae: 1599.0, vali mae: 1693.0, test mae: 1729.0\n",
      "epoch: 32, train mae: 1567.0, vali mae: 1652.0, test mae: 1699.0\n",
      "epoch: 33, train mae: 1469.0, vali mae: 1561.0, test mae: 1617.0\n",
      "epoch: 34, train mae: 1545.0, vali mae: 1635.0, test mae: 1691.0\n",
      "epoch: 35, train mae: 1428.0, vali mae: 1529.0, test mae: 1578.0\n",
      "epoch: 36, train mae: 1329.0, vali mae: 1419.0, test mae: 1468.0\n",
      "epoch: 37, train mae: 1437.0, vali mae: 1521.0, test mae: 1573.0\n",
      "epoch: 38, train mae: 1292.0, vali mae: 1389.0, test mae: 1437.0\n",
      "epoch: 39, train mae: 1329.0, vali mae: 1415.0, test mae: 1472.0\n",
      "epoch: 40, train mae: 1585.0, vali mae: 1669.0, test mae: 1704.0\n",
      "epoch: 41, train mae: 1320.0, vali mae: 1406.0, test mae: 1446.0\n",
      "epoch: 42, train mae: 1272.0, vali mae: 1370.0, test mae: 1408.0\n",
      "epoch: 43, train mae: 1194.0, vali mae: 1294.0, test mae: 1328.0\n",
      "epoch: 44, train mae: 1165.0, vali mae: 1264.0, test mae: 1292.0\n",
      "epoch: 45, train mae: 1172.0, vali mae: 1263.0, test mae: 1310.0\n",
      "epoch: 46, train mae: 1066.0, vali mae: 1171.0, test mae: 1210.0\n",
      "epoch: 47, train mae: 1145.0, vali mae: 1238.0, test mae: 1284.0\n",
      "epoch: 48, train mae: 978.0, vali mae: 1085.0, test mae: 1131.0\n",
      "epoch: 49, train mae: 1020.0, vali mae: 1123.0, test mae: 1164.0\n",
      "epoch: 50, train mae: 1017.0, vali mae: 1119.0, test mae: 1163.0\n",
      "epoch: 51, train mae: 970.0, vali mae: 1079.0, test mae: 1119.0\n",
      "epoch: 52, train mae: 960.0, vali mae: 1065.0, test mae: 1107.0\n",
      "epoch: 53, train mae: 909.0, vali mae: 1022.0, test mae: 1063.0\n",
      "epoch: 54, train mae: 903.0, vali mae: 1009.0, test mae: 1059.0\n",
      "epoch: 55, train mae: 889.0, vali mae: 998.0, test mae: 1043.0\n",
      "epoch: 56, train mae: 888.0, vali mae: 998.0, test mae: 1047.0\n",
      "epoch: 57, train mae: 881.0, vali mae: 992.0, test mae: 1036.0\n",
      "epoch: 58, train mae: 895.0, vali mae: 1004.0, test mae: 1046.0\n",
      "epoch: 59, train mae: 880.0, vali mae: 985.0, test mae: 1034.0\n",
      "epoch: 60, train mae: 969.0, vali mae: 1052.0, test mae: 1120.0\n",
      "epoch: 61, train mae: 950.0, vali mae: 1043.0, test mae: 1088.0\n",
      "epoch: 62, train mae: 902.0, vali mae: 1000.0, test mae: 1052.0\n",
      "epoch: 63, train mae: 858.0, vali mae: 962.0, test mae: 1013.0\n",
      "epoch: 64, train mae: 886.0, vali mae: 980.0, test mae: 1025.0\n",
      "epoch: 65, train mae: 821.0, vali mae: 932.0, test mae: 975.0\n",
      "epoch: 66, train mae: 820.0, vali mae: 933.0, test mae: 972.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 67, train mae: 800.0, vali mae: 907.0, test mae: 956.0\n",
      "epoch: 68, train mae: 783.0, vali mae: 894.0, test mae: 941.0\n",
      "epoch: 69, train mae: 773.0, vali mae: 889.0, test mae: 932.0\n",
      "epoch: 70, train mae: 772.0, vali mae: 888.0, test mae: 930.0\n",
      "epoch: 71, train mae: 776.0, vali mae: 885.0, test mae: 932.0\n",
      "epoch: 72, train mae: 753.0, vali mae: 870.0, test mae: 914.0\n",
      "epoch: 73, train mae: 749.0, vali mae: 865.0, test mae: 910.0\n",
      "epoch: 74, train mae: 750.0, vali mae: 861.0, test mae: 912.0\n",
      "epoch: 75, train mae: 743.0, vali mae: 859.0, test mae: 901.0\n",
      "epoch: 76, train mae: 736.0, vali mae: 851.0, test mae: 896.0\n",
      "epoch: 77, train mae: 743.0, vali mae: 856.0, test mae: 902.0\n",
      "epoch: 78, train mae: 728.0, vali mae: 844.0, test mae: 889.0\n",
      "epoch: 79, train mae: 735.0, vali mae: 849.0, test mae: 893.0\n",
      "epoch: 80, train mae: 766.0, vali mae: 876.0, test mae: 919.0\n",
      "epoch: 81, train mae: 740.0, vali mae: 856.0, test mae: 898.0\n",
      "epoch: 82, train mae: 735.0, vali mae: 851.0, test mae: 895.0\n",
      "epoch: 83, train mae: 727.0, vali mae: 842.0, test mae: 886.0\n",
      "epoch: 84, train mae: 715.0, vali mae: 828.0, test mae: 877.0\n",
      "epoch: 85, train mae: 713.0, vali mae: 829.0, test mae: 873.0\n",
      "epoch: 86, train mae: 710.0, vali mae: 825.0, test mae: 870.0\n",
      "epoch: 87, train mae: 705.0, vali mae: 822.0, test mae: 867.0\n",
      "epoch: 88, train mae: 701.0, vali mae: 818.0, test mae: 864.0\n",
      "epoch: 89, train mae: 699.0, vali mae: 817.0, test mae: 863.0\n",
      "epoch: 90, train mae: 696.0, vali mae: 816.0, test mae: 858.0\n",
      "epoch: 91, train mae: 695.0, vali mae: 812.0, test mae: 857.0\n",
      "epoch: 92, train mae: 693.0, vali mae: 811.0, test mae: 855.0\n",
      "epoch: 93, train mae: 690.0, vali mae: 809.0, test mae: 852.0\n",
      "epoch: 94, train mae: 690.0, vali mae: 808.0, test mae: 853.0\n",
      "epoch: 95, train mae: 688.0, vali mae: 807.0, test mae: 851.0\n",
      "epoch: 96, train mae: 687.0, vali mae: 806.0, test mae: 849.0\n",
      "epoch: 97, train mae: 685.0, vali mae: 802.0, test mae: 848.0\n",
      "epoch: 98, train mae: 684.0, vali mae: 802.0, test mae: 846.0\n",
      "epoch: 99, train mae: 682.0, vali mae: 800.0, test mae: 844.0\n",
      "Saved-Not a good approximation!\n",
      "Exact: [48.63636364 43.18181818 40.90909091 47.5        47.5        47.5\n",
      " 48.63636364 40.90909091 45.         36.98113208 45.         45.\n",
      " 45.         45.         36.98113208 40.          0.         49.\n",
      " 49.         49.         48.63636364 50.          0.         50.        ]\n",
      "Estimated sample: [48.04276657 42.12129424 41.5476764  46.96324158 46.99454117 48.05492401\n",
      " 47.28795624 41.5476764  45.0649147  39.17285043 44.26655197 45.14894485\n",
      " 45.28691483 44.82571793 39.17285043 40.19895554  0.         49.07095337\n",
      " 48.80304337 48.59174728 47.28795624 50.          0.         50.        ]\n",
      "TD length: 10000\n",
      "epoch: 0, train mae: 27530.0, vali mae: 27452.0, test mae: 27571.0\n",
      "epoch: 1, train mae: 9287.0, vali mae: 9384.0, test mae: 9315.0\n",
      "epoch: 2, train mae: 9013.0, vali mae: 9113.0, test mae: 9054.0\n",
      "epoch: 3, train mae: 8471.0, vali mae: 8564.0, test mae: 8516.0\n",
      "epoch: 4, train mae: 8048.0, vali mae: 8141.0, test mae: 8093.0\n",
      "epoch: 5, train mae: 6351.0, vali mae: 6424.0, test mae: 6377.0\n",
      "epoch: 6, train mae: 4891.0, vali mae: 4918.0, test mae: 4930.0\n",
      "epoch: 7, train mae: 4509.0, vali mae: 4554.0, test mae: 4532.0\n",
      "epoch: 8, train mae: 4384.0, vali mae: 4491.0, test mae: 4421.0\n",
      "epoch: 9, train mae: 4044.0, vali mae: 4116.0, test mae: 4076.0\n",
      "epoch: 10, train mae: 3506.0, vali mae: 3584.0, test mae: 3553.0\n",
      "epoch: 11, train mae: 3300.0, vali mae: 3386.0, test mae: 3338.0\n",
      "epoch: 12, train mae: 3285.0, vali mae: 3378.0, test mae: 3337.0\n",
      "epoch: 13, train mae: 2950.0, vali mae: 3033.0, test mae: 3003.0\n",
      "epoch: 14, train mae: 2922.0, vali mae: 3017.0, test mae: 2960.0\n",
      "epoch: 15, train mae: 2803.0, vali mae: 2869.0, test mae: 2853.0\n",
      "epoch: 16, train mae: 2771.0, vali mae: 2834.0, test mae: 2821.0\n",
      "epoch: 17, train mae: 2645.0, vali mae: 2715.0, test mae: 2694.0\n",
      "epoch: 18, train mae: 2529.0, vali mae: 2577.0, test mae: 2569.0\n",
      "epoch: 19, train mae: 2377.0, vali mae: 2427.0, test mae: 2406.0\n",
      "epoch: 20, train mae: 2748.0, vali mae: 2782.0, test mae: 2787.0\n",
      "epoch: 21, train mae: 2529.0, vali mae: 2568.0, test mae: 2556.0\n",
      "epoch: 22, train mae: 2343.0, vali mae: 2374.0, test mae: 2390.0\n",
      "epoch: 23, train mae: 2015.0, vali mae: 2059.0, test mae: 2063.0\n",
      "epoch: 24, train mae: 1953.0, vali mae: 2004.0, test mae: 2004.0\n",
      "epoch: 25, train mae: 1827.0, vali mae: 1882.0, test mae: 1894.0\n",
      "epoch: 26, train mae: 1770.0, vali mae: 1819.0, test mae: 1819.0\n",
      "epoch: 27, train mae: 1652.0, vali mae: 1718.0, test mae: 1713.0\n",
      "epoch: 28, train mae: 1697.0, vali mae: 1739.0, test mae: 1737.0\n",
      "epoch: 29, train mae: 1610.0, vali mae: 1671.0, test mae: 1662.0\n",
      "epoch: 30, train mae: 1448.0, vali mae: 1500.0, test mae: 1486.0\n",
      "epoch: 31, train mae: 1402.0, vali mae: 1473.0, test mae: 1459.0\n",
      "epoch: 32, train mae: 1503.0, vali mae: 1565.0, test mae: 1547.0\n",
      "epoch: 33, train mae: 1277.0, vali mae: 1345.0, test mae: 1328.0\n",
      "epoch: 34, train mae: 1443.0, vali mae: 1498.0, test mae: 1491.0\n",
      "epoch: 35, train mae: 1328.0, vali mae: 1397.0, test mae: 1381.0\n",
      "epoch: 36, train mae: 1379.0, vali mae: 1422.0, test mae: 1418.0\n",
      "epoch: 37, train mae: 1354.0, vali mae: 1419.0, test mae: 1398.0\n",
      "epoch: 38, train mae: 1205.0, vali mae: 1272.0, test mae: 1254.0\n",
      "epoch: 39, train mae: 1281.0, vali mae: 1358.0, test mae: 1327.0\n",
      "epoch: 40, train mae: 1665.0, vali mae: 1701.0, test mae: 1692.0\n",
      "epoch: 41, train mae: 1381.0, vali mae: 1427.0, test mae: 1417.0\n",
      "epoch: 42, train mae: 1197.0, vali mae: 1259.0, test mae: 1253.0\n",
      "epoch: 43, train mae: 1418.0, vali mae: 1483.0, test mae: 1457.0\n",
      "epoch: 44, train mae: 1297.0, vali mae: 1360.0, test mae: 1346.0\n",
      "epoch: 45, train mae: 1095.0, vali mae: 1175.0, test mae: 1150.0\n",
      "epoch: 46, train mae: 1083.0, vali mae: 1158.0, test mae: 1136.0\n",
      "epoch: 47, train mae: 1083.0, vali mae: 1162.0, test mae: 1139.0\n",
      "epoch: 48, train mae: 1028.0, vali mae: 1098.0, test mae: 1080.0\n",
      "epoch: 49, train mae: 1069.0, vali mae: 1152.0, test mae: 1124.0\n",
      "epoch: 50, train mae: 1028.0, vali mae: 1091.0, test mae: 1081.0\n",
      "epoch: 51, train mae: 941.0, vali mae: 1021.0, test mae: 1000.0\n",
      "epoch: 52, train mae: 1004.0, vali mae: 1070.0, test mae: 1060.0\n",
      "epoch: 53, train mae: 954.0, vali mae: 1039.0, test mae: 1013.0\n",
      "epoch: 54, train mae: 971.0, vali mae: 1036.0, test mae: 1019.0\n",
      "epoch: 55, train mae: 952.0, vali mae: 1021.0, test mae: 1002.0\n",
      "epoch: 56, train mae: 917.0, vali mae: 988.0, test mae: 971.0\n",
      "epoch: 57, train mae: 906.0, vali mae: 976.0, test mae: 962.0\n",
      "epoch: 58, train mae: 910.0, vali mae: 987.0, test mae: 961.0\n",
      "epoch: 59, train mae: 894.0, vali mae: 969.0, test mae: 949.0\n",
      "epoch: 60, train mae: 1138.0, vali mae: 1193.0, test mae: 1178.0\n",
      "epoch: 61, train mae: 932.0, vali mae: 1008.0, test mae: 981.0\n",
      "epoch: 62, train mae: 979.0, vali mae: 1050.0, test mae: 1027.0\n",
      "epoch: 63, train mae: 897.0, vali mae: 974.0, test mae: 952.0\n",
      "epoch: 64, train mae: 885.0, vali mae: 954.0, test mae: 936.0\n",
      "epoch: 65, train mae: 842.0, vali mae: 920.0, test mae: 899.0\n",
      "epoch: 66, train mae: 839.0, vali mae: 919.0, test mae: 894.0\n",
      "epoch: 67, train mae: 864.0, vali mae: 940.0, test mae: 916.0\n",
      "epoch: 68, train mae: 817.0, vali mae: 893.0, test mae: 873.0\n",
      "epoch: 69, train mae: 826.0, vali mae: 905.0, test mae: 879.0\n",
      "epoch: 70, train mae: 817.0, vali mae: 895.0, test mae: 871.0\n",
      "epoch: 71, train mae: 804.0, vali mae: 880.0, test mae: 859.0\n",
      "epoch: 72, train mae: 796.0, vali mae: 875.0, test mae: 849.0\n",
      "epoch: 73, train mae: 783.0, vali mae: 864.0, test mae: 838.0\n",
      "epoch: 74, train mae: 788.0, vali mae: 865.0, test mae: 844.0\n",
      "epoch: 75, train mae: 794.0, vali mae: 873.0, test mae: 847.0\n",
      "epoch: 76, train mae: 774.0, vali mae: 853.0, test mae: 830.0\n",
      "epoch: 77, train mae: 787.0, vali mae: 863.0, test mae: 838.0\n",
      "epoch: 78, train mae: 776.0, vali mae: 854.0, test mae: 830.0\n",
      "epoch: 79, train mae: 774.0, vali mae: 848.0, test mae: 828.0\n",
      "epoch: 80, train mae: 814.0, vali mae: 887.0, test mae: 862.0\n",
      "epoch: 81, train mae: 772.0, vali mae: 850.0, test mae: 823.0\n",
      "epoch: 82, train mae: 770.0, vali mae: 847.0, test mae: 821.0\n",
      "epoch: 83, train mae: 767.0, vali mae: 840.0, test mae: 817.0\n",
      "epoch: 84, train mae: 759.0, vali mae: 835.0, test mae: 811.0\n",
      "epoch: 85, train mae: 756.0, vali mae: 835.0, test mae: 809.0\n",
      "epoch: 86, train mae: 751.0, vali mae: 829.0, test mae: 804.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 87, train mae: 746.0, vali mae: 824.0, test mae: 798.0\n",
      "epoch: 88, train mae: 749.0, vali mae: 827.0, test mae: 800.0\n",
      "epoch: 89, train mae: 746.0, vali mae: 822.0, test mae: 798.0\n",
      "epoch: 90, train mae: 739.0, vali mae: 818.0, test mae: 792.0\n",
      "epoch: 91, train mae: 737.0, vali mae: 815.0, test mae: 789.0\n",
      "epoch: 92, train mae: 734.0, vali mae: 813.0, test mae: 788.0\n",
      "epoch: 93, train mae: 734.0, vali mae: 811.0, test mae: 786.0\n",
      "epoch: 94, train mae: 731.0, vali mae: 809.0, test mae: 785.0\n",
      "epoch: 95, train mae: 729.0, vali mae: 808.0, test mae: 782.0\n",
      "epoch: 96, train mae: 729.0, vali mae: 808.0, test mae: 781.0\n",
      "epoch: 97, train mae: 727.0, vali mae: 804.0, test mae: 779.0\n",
      "epoch: 98, train mae: 726.0, vali mae: 805.0, test mae: 778.0\n",
      "epoch: 99, train mae: 726.0, vali mae: 807.0, test mae: 778.0\n",
      "Saved-Not a good approximation!\n",
      "Exact: [48.63636364 43.18181818 40.90909091 47.5        47.5        47.5\n",
      " 48.63636364 40.90909091 45.         36.98113208 45.         45.\n",
      " 45.         45.         36.98113208 40.          0.         49.\n",
      " 49.         49.         48.63636364 50.          0.         50.        ]\n",
      "Estimated sample: [47.80130005 43.06583318 41.55709647 47.46234894 47.99705887 47.59664536\n",
      " 46.64613342 41.55709647 45.35263062 38.19787161 45.18465042 45.125103\n",
      " 45.51130676 45.18665695 38.19787161 39.53139114  0.         47.9908638\n",
      " 49.31204224 47.9354744  46.64613342 50.          0.         50.        ]\n",
      "TD length: 20000\n",
      "epoch: 0, train mae: 9031.0, vali mae: 9165.0, test mae: 9036.0\n",
      "epoch: 1, train mae: 8288.0, vali mae: 8448.0, test mae: 8271.0\n",
      "epoch: 2, train mae: 6901.0, vali mae: 7029.0, test mae: 6886.0\n",
      "epoch: 3, train mae: 5018.0, vali mae: 5074.0, test mae: 4962.0\n",
      "epoch: 4, train mae: 3895.0, vali mae: 3957.0, test mae: 3937.0\n",
      "epoch: 5, train mae: 3557.0, vali mae: 3607.0, test mae: 3605.0\n",
      "epoch: 6, train mae: 3403.0, vali mae: 3496.0, test mae: 3473.0\n",
      "epoch: 7, train mae: 3050.0, vali mae: 3127.0, test mae: 3111.0\n",
      "epoch: 8, train mae: 2636.0, vali mae: 2700.0, test mae: 2695.0\n",
      "epoch: 9, train mae: 2582.0, vali mae: 2646.0, test mae: 2627.0\n",
      "epoch: 10, train mae: 2435.0, vali mae: 2499.0, test mae: 2483.0\n",
      "epoch: 11, train mae: 2308.0, vali mae: 2352.0, test mae: 2341.0\n",
      "epoch: 12, train mae: 2094.0, vali mae: 2141.0, test mae: 2133.0\n",
      "epoch: 13, train mae: 2134.0, vali mae: 2172.0, test mae: 2171.0\n",
      "epoch: 14, train mae: 1977.0, vali mae: 2028.0, test mae: 2026.0\n",
      "epoch: 15, train mae: 1888.0, vali mae: 1943.0, test mae: 1935.0\n",
      "epoch: 16, train mae: 1871.0, vali mae: 1928.0, test mae: 1919.0\n",
      "epoch: 17, train mae: 1792.0, vali mae: 1844.0, test mae: 1831.0\n",
      "epoch: 18, train mae: 1713.0, vali mae: 1751.0, test mae: 1756.0\n",
      "epoch: 19, train mae: 1581.0, vali mae: 1631.0, test mae: 1640.0\n",
      "epoch: 20, train mae: 2358.0, vali mae: 2415.0, test mae: 2407.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-6c4b54815a2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mValue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'V'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m#Training NN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mTrain_NN_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mexperiments\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mNN_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'NN_trained.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-e676214d1d84>\u001b[0m in \u001b[0;36mTrain_NN_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;31m##########################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_mae\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPolicy_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValue_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvali_mae\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPolicy_vali\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValue_vali\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_mae\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPolicy_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValue_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1359\u001b[0m                                          \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m                                          \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m                                          callbacks=callbacks)\n\u001b[0m\u001b[0;32m   1362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m     def predict(self, x,\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    441\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m                 \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices_for_conversion_to_dense\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[1;34m(arrays, start, stop)\u001b[0m\n\u001b[0;32m    553\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    553\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Exact_data = []\n",
    "Estimated_data = []\n",
    "exact_counter = 0\n",
    "for TDlen_counter in range(0,len(Tdata_len)):\n",
    "    TD_len = Tdata_len[TDlen_counter]*1000\n",
    "    print('TD length:',TD_len)\n",
    "    Values_Exact = value_iteration_main(exact_apprx_flag = 1)\n",
    "    Exact_data.append(Values_Exact)\n",
    "    \n",
    "    Estimated_data_tmp = []\n",
    "    #Generating training data\n",
    "    TrainingDataObj = trainingData(entry_points, destination, intermediate_destinations, FN, FP, beta, IFG, SS, Action_set_DIFT, Action_set_APT)\n",
    "    pecent_stochastic_DIFT = 40 #Specify percentage of stochastic policies generated for DIFT\n",
    "    pecent_stochastic_APT = 0 #Specify percentage of stochastic policies generated for APT\n",
    "    Input_Output_Data = TrainingDataObj.Generate_Training_Data(TD_len,pecent_stochastic_DIFT,pecent_stochastic_APT) # '0': Input Data for NN ; '1': Output data for NN\n",
    "    np.savez('Data.npz', PdPa=Input_Output_Data[0], V=Input_Output_Data[1])\n",
    "    data = np.load('Data.npz')\n",
    "    Policy = data['PdPa']\n",
    "    Value = data['V']\n",
    "    #Training NN\n",
    "    Train_NN_model()\n",
    "    for experiments in range(0,100):\n",
    "        NN_model = load_model('NN_trained.h5')\n",
    "        Values_Approximated = value_iteration_main(exact_apprx_flag = 0)\n",
    "        Estimated_data_tmp.append(Values_Approximated) \n",
    "        \n",
    "    Estimated_data.append(Estimated_data_tmp)\n",
    "    print('Exact:',Exact_data[exact_counter])\n",
    "    print('Estimated sample:',Estimated_data[exact_counter][4])\n",
    "    exact_counter = exact_counter + 1\n",
    "    \n",
    "np.savez('Plot_Data_Tdata2.npz', Exact_data = Exact_data, Estimated_data = Estimated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.845889013669639, 7.681645752576213, 1.3461621866202966, 1.0439512622713747, 0.5647599054435374, 0.34987352928921683]\n",
      "[0.38795262831974275, 0.10898064915359591, 0.07653336295077491, 0.06562260512342916, 0.03526584585688203, 0.02838151163210038]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "#mean absolute error calculations\n",
    "from statistics import mean \n",
    "import math\n",
    "import statistics\n",
    "MAE = [[],[]] \n",
    "\n",
    "for len_TD in range(0,len(Exact_data)):\n",
    "    MAE_tmp = []\n",
    "    for ii in range(0,100):\n",
    "        diff = Exact_data[len_TD] - Estimated_data[len_TD][ii]\n",
    "        abs_value =  [abs(ele) for ele in diff] \n",
    "        mae = mean(abs_value)\n",
    "        MAE_tmp.append(mae)\n",
    "\n",
    "    mean_MAE_tmp = mean(MAE_tmp)\n",
    "    std_err_MAE_tmp = (statistics.stdev(MAE_tmp))/math.sqrt(10)\n",
    "    \n",
    "    MAE[0].append(mean_MAE_tmp)\n",
    "    MAE[1].append(std_err_MAE_tmp)\n",
    "\n",
    "print(MAE[0])\n",
    "print(MAE[1])\n",
    "print(len(MAE[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.45793639 7.5726651  1.26962882 0.97832866 0.52949406 0.32149202]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as pl\n",
    "import numpy as np\n",
    "\n",
    "pl.clf()\n",
    "\n",
    "x = Tdata_len\n",
    "y = MAE[0]\n",
    "error = MAE[1]\n",
    "print(np.array(y)-np.array(error))\n",
    "pl.plot(x, y, 'k', color='black')\n",
    "pl.fill_between(x, (np.array(y)-np.array(error)), (np.array(y)+np.array(error)),\n",
    "    alpha=1, edgecolor='black', facecolor='silver',\n",
    "    linewidth=0)\n",
    "\n",
    "plt.xlabel(r'Number of training data $ (\\times 10^{3})$', fontsize=12)\n",
    "plt.ylabel(r'Mean absolute error $\\mu$', fontsize=12)\n",
    "\n",
    "plt.title('Mean absolute error vs. Number of training data', fontsize=12)\n",
    "plt.grid(which='major', linestyle='--', linewidth='0.5', color='silver')\n",
    "\n",
    "label_size = 10\n",
    "pl.rcParams['xtick.labelsize'] = label_size \n",
    "pl.rcParams['ytick.labelsize'] = label_size \n",
    "\n",
    "plt.savefig('TD_len.png')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
